{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"TripletMarginLoss-0.2.ipynb","provenance":[{"file_id":"13yQSmVzddAa7-qxOEUtEX3Tt89I6ETpV","timestamp":1639668139292},{"file_id":"https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/TripletMarginLossMNIST.ipynb","timestamp":1639668015260}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lMde13VLDjiH","outputId":"d19a4105-89b7-4be8-cac1-cc0551b5c720","executionInfo":{"status":"ok","timestamp":1639669968154,"user_tz":-60,"elapsed":14434,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"}}},"source":["!pip install pytorch-metric-learning\n","!pip install faiss-gpu"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-metric-learning\n","  Downloading pytorch_metric_learning-1.0.0-py3-none-any.whl (102 kB)\n","\u001b[?25l\r\u001b[K     |███▏                            | 10 kB 33.5 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 30 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 40 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 61 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 71 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 92 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 102 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102 kB 7.7 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.10.0+cu111)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.0.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.19.5)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (0.11.1+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.10.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (3.0.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (1.1.0)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch-metric-learning) (7.1.2)\n","Installing collected packages: pytorch-metric-learning\n","Successfully installed pytorch-metric-learning-1.0.0\n","Collecting faiss-gpu\n","  Downloading faiss_gpu-1.7.1.post3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (90.1 MB)\n","\u001b[K     |████████████████████████████████| 90.1 MB 23 kB/s \n","\u001b[?25hInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.1.post3\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"GJ_L0TrTDnEA","outputId":"2a0ca948-bbfb-47fa-e58a-90e43eccadcd","executionInfo":{"status":"error","timestamp":1639671102395,"user_tz":-60,"elapsed":100633,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","\n","### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n","from torchvision import datasets, transforms\n","\n","from pytorch_metric_learning import distances, losses, miners, reducers, testers\n","from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n","\n","######################## Timo's Code ########################\n","import pandas as pd\n","import numpy as np\n","import PIL\n","import os\n","from os.path import isfile, join\n","from google.colab import drive\n","import toml\n","\n","class FAUPapyrusCollectionDataset(torch.utils.data.Dataset):\n","    \"\"\"FAUPapyrusCollection dataset.\"\"\"\n","    def __init__(self, root_dir, processed_frame, transform=None):\n","\n","        self.root_dir = root_dir\n","        self.processed_frame = processed_frame\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.processed_frame)       \n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.processed_frame.iloc[idx, 1])\n","        \n","        img_name = img_name + '.png'\n","        \n","        #image = io.imread(img_name , plugin='matploPILtlib')        \n","        image = PIL.Image.open(img_name)\n","        if self.transform:\n","            image = self.transform(image)         \n","\n","        papyID = self.processed_frame.iloc[idx,3]\n","\n","\n","        return image, papyID\n","########################################################################    \n","\n","\n","### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.dropout1 = nn.Dropout2d(0.25)\n","        self.dropout2 = nn.Dropout2d(0.5)\n","        self.fc1 = nn.Linear(12544, 128)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        return \n","        \n","def plot_acc(val_precision_at_1_values, epochs, output_path):  \n","  epochs = np.arange(1, epochs + 1)\n","  plt.style.use('seaborn')\n","  width = 460\n","  tex_fonts = {\n","      # Use LaTeX to write all text\n","      \"text.usetex\": True,\n","      \"font.family\": \"serif\",\n","      # Use 10pt font in plots, to match 10pt font in document\n","      \"axes.labelsize\": 10,\n","      \"font.size\": 10,\n","      # Make the legend/label fonts a little smaller\n","      \"legend.fontsize\": 8,\n","      \"xtick.labelsize\": 8,\n","      \"ytick.labelsize\": 8,\n","      \"legend.loc\":'lower left'\n","  }\n","  plt.rcParams.update(tex_fonts)\n","  \n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.plot(epochs, val_precision_at_1_values, 'm', label='Val P@1', linestyle='dotted', linewidth=.3)\n","\n","\n","  ax.set_xlabel('Epochs')\n","  ax.set_ylabel('Accuracy')\n","  ax.legend()\n","  fig.savefig(output_path + '/acc.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()\n","\n","def plot_table(setting, param, dml_param, output_path):  \n","  ########## Plot Settings ##################\n","  setting_name_list = list(setting.keys())\n","  setting_value_list = list(setting.values())\n","  setting_name_list, setting_value_list = replace_helper(setting_name_list, setting_value_list)\n","  vals = np.array([setting_name_list, setting_value_list], dtype=str).T\n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.table(cellText=vals, colLabels=['Setting', 'Value'], loc='center', zorder=3, rowLoc='left', cellLoc='left')\n","  ax.set_title('Experiment Settings')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  fig.savefig(output_path + '/settings.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()\n","\n","  ########## Plot Params ##################\n","  param_name_list = param.keys()\n","  param_value_list = param.values()\n","  param_name_list, param_value_list = replace_helper(param_name_list, param_value_list)\n","  param_vals = np.array([list(param_name_list), list(param_value_list)], dtype=str).T\n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.table(cellText=param_vals, colLabels=['Hyperparameter', 'Value'], loc='center', zorder=3, rowLoc='left', cellLoc='left')\n","  ax.set_title('Hyperparaeters')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  fig.savefig(output_path + '/params.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()\n","\n","  ########## Plot DML Params ##################\n","  dml_param_name_list = dml_param.keys()\n","  dml_param_value_list = dml_param.values()\n","  dml_param_name_list, dml_param_value_list = replace_helper(dml_param_name_list, dml_param_value_list)\n","  dml_param_vals = np.array([list(dml_param_name_list), list(dml_param_value_list)], dtype=str).T  \n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.table(cellText=dml_param_vals, colLabels=['DML Hyperparameter', 'Value'], loc='center', zorder=3, rowLoc='left', cellLoc='left')\n","  ax.set_title('DML Hyperparameters')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  fig.savefig(output_path + '/dml_params.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()\n","\n","def create_output_dir(name, experiment_name, x=1):\n","  while True:\n","        dir_name = (name + (str(x) + '_iteration_' if x is not 0 else '') + 'of_experiment_' + experiment_name).strip()\n","        if not os.path.exists(dir_name):\n","            os.mkdir(dir_name)            \n","\n","            return dir_name\n","        else:\n","            x = x + 1\n","\n","def create_logging(setting, param, dml_param, train_losses, test_precisions_at_1, epochs, output_dir):\n","  plot_table(setting, param, dml_param, output_dir)\n","  plot_loss(train_losses, epochs, output_path)\n","  plot_acc(test_precisions_at_1, epochs, output_path)\n","  pdfs = ['/gradients.pdf', '/loss.pdf', '/acc.pdf', '/params.pdf','/dml_params.pdf', '/settings.pdf']\n","  bookmarks = ['Gradients', 'Loss', 'Accuracy', 'Hyperparameters','DML Hyperparameters', 'Seetings']\n","  merger = PdfFileMerger()\n","\n","  for i, pdf in enumerate(pdfs):\n","      merger.append(output_dir + pdf, bookmark=bookmarks[i])\n","  \n","  pdf = FPDF()   \n","  pdf.add_page() \n","  pdf.set_font(\"Helvetica\", size = 6)\n","  \n","  f = open(\"log.txt\", \"r\")\n","  for x in f:\n","    pdf.cell(200, 6, txt = x, ln = 1, align = 'l')\n","\n","  pdf.output(\"log.pdf\")   \n","  merger.append(\"log.pdf\", bookmark='Log')\n","  merger.write(output_dir + \"/report.pdf\")\n","  merger.close()  \n","  copyfile('log.txt', output_dir + '/log.txt')\n","\n","def plot_loss(train_losses, epochs, output_path):  \n","  \n","  epochs = np.arange(1, epochs + 1)\n","  train_loss_values = np.array(train_loss_values)\n","  val_loss_values = np.array(val_loss_values)\n","  plt.style.use('seaborn')\n","  width = 460\n","  \n","  if True:\n","    tex_fonts = {\n","        # Use LaTeX to write all text\n","        \"text.usetex\": False,\n","        \"font.family\": \"serif\",\n","        # Use 10pt font in plots, to match 10pt font in document\n","        \"axes.labelsize\": 10,\n","        \"font.size\": 10,\n","        # Make the legend/label fonts a little smaller\n","        \"legend.fontsize\": 8,\n","        \"xtick.labelsize\": 8,\n","        \"ytick.labelsize\": 8,\n","        \"legend.loc\":'lower left'\n","    }\n","\n","  plt.rcParams.update(tex_fonts)\n","  \n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","\n","  # plot original lines\n","  ax.plot(epochs, train_loss_values, 'b', label='Training Loss', linestyle='dotted')\n","  #ax.plot(epochs, val_loss_values, 'g', label='Validation Loss', linestyle='dotted')\n","\n","  \n","  ax.set_title('Training')\n","  ax.set_xlabel('Epochs')\n","  ax.set_ylabel('Loss')\n","  ax.legend()\n","  # Save and remove excess whitespace\n","  fig.savefig(output_path + '/loss.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()\n","\n","\n","### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n","def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n","    model.train()\n","    for batch_idx, (data, labels) in enumerate(train_loader):\n","        data, labels = data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        embeddings = model(data)\n","        indices_tuple = mining_func(embeddings, labels)\n","        loss = loss_func(embeddings, labels, indices_tuple)\n","        loss.backward()\n","        optimizer.step()\n","        if batch_idx % 20 == 0:\n","            print(\n","                \"Epoch {} Iteration {}: Loss = {}, Number of mined triplets = {}\".format(\n","                    epoch, batch_idx, loss, mining_func.num_triplets\n","                )\n","            )\n","        return loss\n","\n","### convenient function from pytorch-metric-learning ###\n","def get_all_embeddings(dataset, model):\n","    tester = testers.BaseTester()\n","    return tester.get_all_embeddings(dataset, model)\n","\n","\n","### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n","def test(train_set, test_set, model, accuracy_calculator):\n","    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n","    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n","    train_labels = train_labels.squeeze(1)\n","    test_labels = test_labels.squeeze(1)\n","    print(\"Computing accuracy\")\n","    accuracies = accuracy_calculator.get_accuracy(\n","        test_embeddings, train_embeddings, test_labels, train_labels, False\n","    )\n","    print(\"Test set accuracy (Precision@1) = {}\".format(accuracies[\"precision_at_1\"]))\n","    return accuracies[\"precision_at_1\"]\n","\n","def gradient_visualization(parameters, output_dir):\n","    \"\"\"\n","    Returns the parameter gradients over the epoch.\n","    :param parameters: parameters of the network\n","    :type parameters: iterator\n","    :param results_folder: path to results folder\n","    :type results_folder: str\n","    \"\"\"\n","    tex_fonts = {\n","    # Use LaTeX to write all text\n","    \"text.usetex\": False,\n","    \"font.family\": \"serif\",\n","    # Use 10pt font in plots, to match 10pt font in document\n","    \"axes.labelsize\": 10,\n","    \"font.size\": 10,\n","    # Make the legend/label fonts a little smaller\n","    \"legend.fontsize\": 8,\n","    \"xtick.labelsize\": 8,\n","    \"ytick.labelsize\": 8,\n","    \"legend.loc\":'lower left'\n","}\n","\n","    plt.rcParams.update(tex_fonts)\n","\n","    ave_grads = []\n","    layers = []\n","\n","\n","    for n, p in parameters:\n","        if (p.requires_grad) and (\"bias\" not in n):\n","            layers.append(n)\n","            ave_grads.append(p.grad.abs().mean())\n","    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color=\"k\")\n","    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(xmin=0, xmax=len(ave_grads))\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient Visualization\")\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.savefig(output_dir + \"/gradients.pdf\")\n","    plt.close()\n","\n","\n","device = torch.device(\"cuda\")\n","\n","transform = transforms.Compose(\n","    [transforms.Resize((32,32)), transforms.Grayscale(),transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",")\n","\n","### Timo's\n","\n","drive.mount('/content/gdrive')\n","\n","def create_processed_info(path, debug=False):\n","  if debug:\n","    info_path = join(path, 'debug_processed_info.csv')\n","  else:\n","    info_path = join(path, 'processed_info.csv')\n","  if isfile(info_path):\n","    processed_frame = pd.read_csv(info_path, index_col=0, dtype={'fnames':str,'papyID':int,'posinfo':str, 'pixelCentimer':float}, header=0)    \n","  else:    \n","    fnames = [f for f in listdir(path) if isfile(join(path, f))]\n","    fnames = [ x for x in fnames if \".png\" in x ]\n","    fnames = [f.split('.',1)[0] for f in fnames]\n","    fnames_frame = pd.DataFrame(fnames,columns=['fnames'])\n","    fragmentID = pd.DataFrame([f.split('_',1)[0] for f in fnames], columns=['fragmentID'])\n","    fnames_raw = [f.split('_',1)[1] for f in fnames]\n","    processed_frame = pd.DataFrame(fnames_raw, columns=['fnames_raw'])\n","    \n","    processed_frame = pd.concat([processed_frame, fnames_frame], axis=1)\n","\n","    processed_frame = pd.concat([processed_frame, fragmentID], axis=1)\n","    processed_frame['papyID'] = processed_frame.fnames_raw.apply(lambda x: x.split('_',1)[0])\n","    processed_frame['posinfo'] = processed_frame.fnames_raw.apply(lambda x: ''.join(filter(str.isalpha, x)))\n","    processed_frame['pixelCentimer'] = processed_frame.fnames_raw.progress_apply(retrive_size_by_fname)\n","    processed_frame.to_csv(info_path)\n","     \n","  return processed_frame\n","\n","config = toml.load('./gdrive/MyDrive/mt/conf/conf.toml')\n","setting = config.get('settings')\n","param = config.get('params')\n","dml_param = config.get('dml_params')\n","\n","output_dir = create_output_dir(setting['output'], setting['experiment_name'])\n","processed_frame_train = create_processed_info(setting['path_train'])\n","processed_frame_val = create_processed_info(setting['path_val'])\n","\n","dataset1 = FAUPapyrusCollectionDataset(setting['path_train'], processed_frame_train, transform)\n","dataset2 = FAUPapyrusCollectionDataset(setting['path_train'], processed_frame_train, transform)\n","\n","batch_size = 256\n","\n","#dataset1 = datasets.MNIST(\".\", train=True, download=True, transform=transform)\n","#dataset2 = datasets.MNIST(\".\", train=False, transform=transform)\n","train_loader = torch.utils.data.DataLoader(dataset1, batch_size=256, shuffle=True, drop_last=True)\n","test_loader = torch.utils.data.DataLoader(dataset2, batch_size=256, drop_last=True)\n","\n","model = Net().to(device)\n","optimizer = optim.Adam(model.parameters(), lr=0.01)\n","num_epochs = 4\n","\n","\n","### pytorch-metric-learning stuff ###\n","distance = distances.CosineSimilarity()\n","reducer = reducers.ThresholdReducer(low=0)\n","loss_func = losses.TripletMarginLoss(margin=0.2, distance=distance, reducer=reducer)\n","mining_func = miners.TripletMarginMiner(\n","    margin=0.2, distance=distance, type_of_triplets=\"semihard\"\n",")\n","accuracy_calculator = AccuracyCalculator(include=(\"precision_at_1\",), k=1)\n","### pytorch-metric-learning stuff ###\n","\n","train_losses = []\n","test_precisions_at_1 = []\n","\n","\n","for epoch in range(1, num_epochs + 1):\n","    train_loss = train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n","    test_precision_at_1 = test(dataset1, dataset2, model, accuracy_calculator)\n","    \n","    train_losses.append(train_loss)\n","    test_precisions_at_1.append(test_precision_at_1)\n","    gradient_visualization(model.named_parameters(), output_dir)\n","    create_logging(setting, param, dml_param, train_losses, test_precisions_at_1, epoch, output_dir)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-ff77c0aed31e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmining_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0mtest_precision_at_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_calculator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-ff77c0aed31e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mindices_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmining_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_tuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_metric_learning/miners/base_miner.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, labels, ref_emb, ref_labels)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mc_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             ref_emb, ref_labels = c_f.set_ref_emb(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_metric_learning/utils/common_functions.py\u001b[0m in \u001b[0;36mcheck_shapes\u001b[0;34m(embeddings, labels)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of embeddings must equal number of labels\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"YDAfn0oVK2zr"},"execution_count":null,"outputs":[]}]}