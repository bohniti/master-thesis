{"cells":[{"cell_type":"markdown","metadata":{"id":"knWdWI4lusVB"},"source":["## Prepare Notebook"]},{"cell_type":"markdown","metadata":{"id":"PDiZVA0UuP9s"},"source":["### Deleate Outdated Log Files"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":36,"status":"ok","timestamp":1642625856678,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"},"user_tz":-60},"id":"XIHVwrwFuMDl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"68d49117-2198-4498-a30b-c6a61fc5d9cb"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove 'log.txt': No such file or directory\n"]}],"source":["!rm \"log.txt\""]},{"cell_type":"markdown","metadata":{"id":"sW4tZpwNhacV"},"source":["### Install Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32020,"status":"ok","timestamp":1642625888678,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"},"user_tz":-60},"id":"lMde13VLDjiH","outputId":"462bd06c-1c5e-4625-ac07-394e68ed9a04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-metric-learning\n","  Downloading pytorch_metric_learning-1.1.0-py3-none-any.whl (106 kB)\n","\u001b[?25l\r\u001b[K     |███                             | 10 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 20 kB 35.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 30 kB 21.1 MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 40 kB 17.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 51 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 71 kB 8.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 81 kB 8.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 106 kB 7.5 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.10.0+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (4.62.3)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (0.11.1+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.0.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.10.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (1.4.1)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch-metric-learning) (7.1.2)\n","Installing collected packages: pytorch-metric-learning\n","Successfully installed pytorch-metric-learning-1.1.0\n","Collecting faiss-gpu\n","  Downloading faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n","\u001b[K     |████████████████████████████████| 85.5 MB 131 kB/s \n","\u001b[?25hInstalling collected packages: faiss-gpu\n","Successfully installed faiss-gpu-1.7.2\n","Collecting PyPDF2\n","  Downloading PyPDF2-1.26.0.tar.gz (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 5.0 MB/s \n","\u001b[?25hBuilding wheels for collected packages: PyPDF2\n","  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-py3-none-any.whl size=61102 sha256=47f56d9510a8f41299944b33fa62e771048b79496ba52e50de49937e6d28a7e5\n","  Stored in directory: /root/.cache/pip/wheels/80/1a/24/648467ade3a77ed20f35cfd2badd32134e96dd25ca811e64b3\n","Successfully built PyPDF2\n","Installing collected packages: PyPDF2\n","Successfully installed PyPDF2-1.26.0\n","Collecting FPDF\n","  Downloading fpdf-1.7.2.tar.gz (39 kB)\n","Building wheels for collected packages: FPDF\n","  Building wheel for FPDF (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for FPDF: filename=fpdf-1.7.2-py2.py3-none-any.whl size=40725 sha256=59f0d0af88068acdaceabfa7ee7f46ee15a9cbbcc2ee39b25e6f47108fd98247\n","  Stored in directory: /root/.cache/pip/wheels/d7/ca/c8/86467e7957bbbcbdf4cf4870fc7dc95e9a16404b2e3c3a98c3\n","Successfully built FPDF\n","Installing collected packages: FPDF\n","Successfully installed FPDF-1.7.2\n","Collecting efficientnet_pytorch\n","  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n","Building wheels for collected packages: efficientnet-pytorch\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16446 sha256=76bea09f9d5c6b3cb26ab1c8d18f5f8ebfc875db6816c8b9ca30a1c2cf28b339\n","  Stored in directory: /root/.cache/pip/wheels/0e/cc/b2/49e74588263573ff778da58cc99b9c6349b496636a7e165be6\n","Successfully built efficientnet-pytorch\n","Installing collected packages: efficientnet-pytorch\n","Successfully installed efficientnet-pytorch-0.7.1\n","Collecting umap-learn\n","  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n","Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n","Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n","Collecting pynndescent>=0.5\n","  Downloading pynndescent-0.5.5.tar.gz (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 65.6 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.62.3)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.0.0)\n","Building wheels for collected packages: umap-learn, pynndescent\n","  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for umap-learn: filename=umap_learn-0.5.2-py3-none-any.whl size=82708 sha256=a1ff4bf7bc04a6502673fd6a077e9ddaa89ea75511dd7eb0fa32fdade69cd5aa\n","  Stored in directory: /root/.cache/pip/wheels/84/1b/c6/aaf68a748122632967cef4dffef68224eb16798b6793257d82\n","  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pynndescent: filename=pynndescent-0.5.5-py3-none-any.whl size=52603 sha256=7419813f7f45d099113eb3f2f51f8492223c70986bd88b287e12e280ead1ab3d\n","  Stored in directory: /root/.cache/pip/wheels/af/e9/33/04db1436df0757c42fda8ea6796d7a8586e23c85fac355f476\n","Successfully built umap-learn pynndescent\n","Installing collected packages: pynndescent, umap-learn\n","Successfully installed pynndescent-0.5.5 umap-learn-0.5.2\n","Collecting gpustat\n","  Downloading gpustat-0.6.0.tar.gz (78 kB)\n","\u001b[K     |████████████████████████████████| 78 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from gpustat) (1.15.0)\n","Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.7/dist-packages (from gpustat) (7.352.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from gpustat) (5.4.8)\n","Collecting blessings>=1.6\n","  Downloading blessings-1.7-py3-none-any.whl (18 kB)\n","Building wheels for collected packages: gpustat\n","  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gpustat: filename=gpustat-0.6.0-py3-none-any.whl size=12617 sha256=8f758b54c7682ddb5324c2500911c2bbb7ad0fab53f3b0cf1fe62136e80ecc07\n","  Stored in directory: /root/.cache/pip/wheels/e6/67/af/f1ad15974b8fd95f59a63dbf854483ebe5c7a46a93930798b8\n","Successfully built gpustat\n","Installing collected packages: blessings, gpustat\n","Successfully installed blessings-1.7 gpustat-0.6.0\n"]}],"source":["!pip install pytorch-metric-learning\n","!pip install faiss-gpu\n","!pip install PyPDF2\n","!pip install FPDF\n","!pip install efficientnet_pytorch\n","!pip install umap-learn\n","!pip install gpustat\n","#!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng"]},{"cell_type":"markdown","metadata":{"id":"rLgbqJQnh3rK"},"source":["### Import Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1642625888679,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"},"user_tz":-60},"id":"8SXtPGdeG_Sg","outputId":"1ea02681-1a49-40e3-f3d5-59aae553b30b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Wed Jan 19 20:58:08 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APm-36xSZmoa"},"outputs":[],"source":["from efficientnet_pytorch import EfficientNet\n","from PyPDF2 import PdfFileMerger\n","from shutil import copyfile\n","from fpdf import FPDF\n","import logging\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import matplotlib.pyplot as plt\n","from torchvision import datasets, transforms\n","from pytorch_metric_learning import distances, losses, miners, reducers, testers\n","from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n","import pandas as pd\n","import numpy as np\n","import PIL\n","import os\n","import toml\n","from os.path import isfile, join\n","from google.colab import drive\n","import matplotlib\n","from matplotlib import rc\n","from sklearn.feature_extraction.image import extract_patches_2d\n","import umap\n","from skimage import io\n","from numpy.core.fromnumeric import mean\n","\n","rc('text', usetex=False)\n","matplotlib.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']"]},{"cell_type":"markdown","metadata":{"id":"izyekehmh-Lq"},"source":["### Mount Google Drive\n","\n","Structure\n","\n","---\n","\n","\n","\n","---\n","\n","\n","* conf\n","* data\n","  * 04_train\n","  * 05_val\n","  * 06_test\n","* out\n","  * experimentName_Iteration"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54213,"status":"ok","timestamp":1642625965484,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"},"user_tz":-60},"id":"rwcJwszEhATV","outputId":"9b3800a7-c82b-4903-9182-3e964537ebf9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"uAOIY2PVukrJ"},"source":["### Instansiate Logger"]},{"cell_type":"markdown","metadata":{"id":"HNiTbsDExczY"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1GTzXo81ucOQ"},"outputs":[],"source":["#logging.basicConfig(filename=\"test.log\", level=logging.INFO )\n","logger = logging.getLogger('log')\n","logger.setLevel(logging.DEBUG)\n","# create file handler which logs even debug messages\n","fh = logging.FileHandler('log.txt')\n","fh.setLevel(logging.INFO)\n","# create console handler with a higher log level\n","ch = logging.StreamHandler()\n","ch.setLevel(logging.INFO)\n","# create formatter and add it to the handlers\n","formatter = logging.Formatter('%(message)s')\n","ch.setFormatter(formatter)\n","fh.setFormatter(formatter)\n","# add the handlers to logger\n","logger.addHandler(ch)\n","logger.addHandler(fh)\n","logger.propagate = False"]},{"cell_type":"markdown","metadata":{"id":"FQ0FoHSgj80L"},"source":["## PyTorch Dataset Class for FAU Papyri Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aI3esmlAZnBR"},"outputs":[],"source":["class FAUPapyrusCollectionDataset(torch.utils.data.Dataset):\n","    \"\"\"FAUPapyrusCollection dataset.\"\"\"\n","    def __init__(self, root_dir, processed_frame, transform=None):\n","\n","        self.root_dir = root_dir\n","        self.processed_frame = processed_frame\n","        self.transform = transform\n","        self.targets = processed_frame[\"papyID\"].unique()\n","\n","    def __len__(self):\n","        return len(self.processed_frame)       \n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.processed_frame.iloc[idx, 1])\n","        \n","        img_name = img_name + '.png'\n","        \n","        image = io.imread(img_name)        \n","        #image = PIL.Image.open(img_name)\n","        \n","        if self.transform:\n","            image = self.transform(image)         \n","\n","        papyID = self.processed_frame.iloc[idx,3]\n","\n","\n","        return image, papyID"]},{"cell_type":"markdown","metadata":{"id":"L887pcZKkIp9"},"source":["## PyTorch Network Architectures"]},{"cell_type":"markdown","metadata":{"id":"fPA-K-A1mPLz"},"source":["### Simple CNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQKpzor1Zq6J"},"outputs":[],"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.dropout1 = nn.Dropout2d(0.25)\n","        self.dropout2 = nn.Dropout2d(0.5)\n","        self.fc1 = nn.Linear(12544, 128)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"6ixxhU2LkVDL"},"source":["## PyTorch NN Functions"]},{"cell_type":"markdown","metadata":{"id":"dAV1rvpSl8g4"},"source":["### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HP0X7F9oZvzY"},"outputs":[],"source":["def train(model, loss_func, mining_func, device, train_loader, optimizer, train_set, epoch, accuracy_calculator, scheduler, accumulation_steps):\n","    model.train()\n","    model.zero_grad()  \n","    epoch_loss = 0.0\n","    running_loss = 0.0\n","    accumulation_steps = 2\n","    for batch_idx, (input_imgs, labels) in enumerate(train_loader):\n","      labels = labels.to(device)\n","      input_imgs = input_imgs.to(device)\n","      bs, ncrops, c, h, w = input_imgs.size()\n","      #optimizer.zero_grad()\n","      \n","      embeddings = model(input_imgs.view(-1, c, h, w)) \n","      embeddings_avg = embeddings.view(bs, ncrops, -1).mean(1) \n","\n","      #Use this if you have to check embedding size\n","      #embedding_size = embeddings_avg.shape\n","      #print(embedding_size)\n","      \n","      indices_tuple = mining_func(embeddings_avg, labels)\n","      loss = loss_func(embeddings_avg, labels, indices_tuple)\n","      loss = loss / accumulation_steps              \n","      loss.backward()\n","\n","      if (batch_idx+1) % accumulation_steps == 0:  \n","        optimizer.step()\n","        optimizer.zero_grad()\n","      \n","      #optimizer.step()\n","      epoch_loss += embeddings_avg.shape[0] * loss.item()\n","      \n","    scheduler.step()\n","    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n","    train_labels = train_labels.squeeze(1)\n","\n","    accuracies = accuracy_calculator.get_accuracy(\n","        train_embeddings,\n","        train_embeddings,\n","        train_labels,\n","        train_labels,\n","        False)\n","\n","    #mean_loss = torch.mean(torch.stack(batch_loss_values))  \n","    logger.info(f\"Epoch {epoch} averg loss from {batch_idx} batches: {epoch_loss}\")\n","    map = accuracies[\"mean_average_precision\"]\n","    logger.info(f\"Eoch {epoch} maP: {map}\")\n","    return epoch_loss, accuracies[\"mean_average_precision\"]"]},{"cell_type":"markdown","metadata":{"id":"bm6s339JkoMv"},"source":["### Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dgh58woPZ2a_"},"outputs":[],"source":["def val(train_set, test_set, model, accuracy_calculator):\n","  train_embeddings, train_labels = get_all_embeddings(train_set, model)\n","  \n","  test_embeddings, test_labels = get_all_embeddings(test_set, model)\n","\n","  train_labels = train_labels.squeeze(1)\n","  test_labels = test_labels.squeeze(1)\n","\n","  print(\"Computing accuracy\")\n","  \n","  accuracies = accuracy_calculator.get_accuracy(\n","      test_embeddings, train_embeddings, test_labels, train_labels, False\n","  )\n","\n","  idx = torch.randperm(test_labels.nelement())\n","  test_labels = test_labels.view(-1)[idx].view(test_labels.size())\n","\n","\n","  random_accuracies = accuracy_calculator.get_accuracy(\n","      test_embeddings, train_embeddings, test_labels, train_labels, False\n","  )\n","\n","  \n","  map = accuracies[\"mean_average_precision\"]\n","  random_map = random_accuracies[\"mean_average_precision\"]\n","  logger.info(f\"Val mAP = {map}\")\n","  logger.info(f\"Val random mAP) = {random_map}\")\n","  \n","  return accuracies[\"mean_average_precision\"], random_accuracies[\"mean_average_precision\"]\n","  "]},{"cell_type":"markdown","metadata":{"id":"VqBveDfpk9bP"},"source":["## Python-Helper-Functions"]},{"cell_type":"markdown","metadata":{"id":"rEC1ONAMmAlg"},"source":["### Deep Metric Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FJiooqcLZxuI"},"outputs":[],"source":["from pytorch_metric_learning.testers import GlobalEmbeddingSpaceTester\n","from pytorch_metric_learning.utils import common_functions as c_f\n","\n","class CustomTester(GlobalEmbeddingSpaceTester):\n","    def get_embeddings_for_eval(self, trunk_model, embedder_model, input_imgs):\n","        input_imgs = c_f.to_device(\n","            input_imgs, device=self.data_device, dtype=self.dtype\n","        )\n","        print('yes')\n","        # from https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.FiveCrop\n","        bs, ncrops, c, h, w = input_imgs.size()\n","        result = embedder_model(trunk_model(input_imgs.view(-1, c, h, w))) # fuse batch size and ncrops\n","        result_avg = result.view(bs, ncrops, -1).mean(1) # avg over crops\n","        return result_avg \n","\n","def visualizer_hook(umapper, umap_embeddings, labels, split_name, keyname, *args):\n","    logging.info(\n","        \"UMAP plot for the {} split and label set {}\".format(split_name, keyname)\n","    )\n","    label_set = np.unique(labels)\n","    num_classes = len(label_set)\n","    fig = plt.figure(figsize=(20, 15))\n","    plt.gca().set_prop_cycle(\n","        cycler(\n","            \"color\", [plt.cm.nipy_spectral(i) for i in np.linspace(0, 0.9, num_classes)]\n","        )\n","    )\n","    for i in range(num_classes):\n","        idx = labels == label_set[i]\n","        plt.plot(umap_embeddings[idx, 0], umap_embeddings[idx, 1], \".\", markersize=1)\n","    plt.show()\n","\n","def get_all_embeddings(dataset, model, collate_fn=None, eval=True):\n","    tester = CustomTester(visualizer=umap.UMAP(),visualizer_hook=visualizer_hook,)\n","    return tester.get_all_embeddings(dataset, model, collate_fn=None)"]},{"cell_type":"markdown","metadata":{"id":"jz6RlvPUlNsI"},"source":["### Visualization"]},{"cell_type":"markdown","metadata":{"id":"bkdqDbrGlYJ5"},"source":["#### Gradients"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wqmF1tTZ79v"},"outputs":[],"source":["def gradient_visualization(parameters, output_path):\n","    \"\"\"\n","    Returns the parameter gradients over the epoch.\n","    :param parameters: parameters of the network\n","    :type parameters: iterator\n","    :param results_folder: path to results folder\n","    :type results_folder: str\n","    \"\"\"\n","    tex_fonts = {\n","    # Use LaTeX to write all text\n","    \"text.usetex\": False,\n","    \"font.family\": \"serif\",\n","    # Use 10pt font in plots, to match 10pt font in document\n","    \"axes.labelsize\": 10,\n","    \"font.size\": 10,\n","    # Make the legend/label fonts a little smaller\n","    \"legend.fontsize\": 8,\n","    \"xtick.labelsize\": 8,\n","    \"ytick.labelsize\": 8,\n","    \"legend.loc\":'lower left'\n","}\n","    plt.rcParams.update(tex_fonts)\n","    ave_grads = []\n","    layers = []\n","    for n, p in parameters:        \n","        if (p.requires_grad) and (\"bias\" not in n):\n","            layers.append(n)\n","            ave_grads.append(p.grad.abs().mean())\n","    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color=\"k\")\n","    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(xmin=0, xmax=len(ave_grads))\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient Visualization\")\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.savefig(output_path + \"/gradients.pdf\")\n","    plt.close()"]},{"cell_type":"markdown","metadata":{"id":"e2m1rZt_rbic"},"source":["#### Accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C2fu_vZFrjzg"},"outputs":[],"source":["def plot_acc(map_vals, random_map_vals, train_map, epochs, output_path):  \n","  width = 460\n","  plt.style.use('seaborn-bright')\n","  tex_fonts = {\n","        # Use LaTeX to write all text\n","        \"text.usetex\": False,\n","        \"font.family\": \"serif\",\n","        # Use 10pt font in plots, to match 10pt font in document\n","        \"axes.labelsize\": 10,\n","        \"font.size\": 10,\n","        # Make the legend/label fonts a little smaller\n","        \"legend.fontsize\": 8,\n","        \"xtick.labelsize\": 8,\n","        \"ytick.labelsize\": 8\n","    }\n","  #linestyle='dotted'\n","\n","  plt.rcParams.update(tex_fonts)  \n","  epochs = np.arange(1, epochs + 1)\n","  fig, ax = plt.subplots(1, 1,figsize=set_size(width))\n","  ax.plot(epochs, random_map_vals, 'r', label='random mAP')\n","  ax.plot(epochs, train_map, 'g', label='train mAP')\n","  ax.plot(epochs, map_vals, 'b', label='val mAP')\n","  ax.set_title('Validation Accuracy')\n","  ax.set_xlabel('Epochs')\n","  ax.set_ylabel('Accuracy')\n","  ax.legend()\n","  fig.savefig(output_path + '/acc.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()"]},{"cell_type":"markdown","metadata":{"id":"cHsBgJs4lIaO"},"source":["#### Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_CyEFtObZ-a"},"outputs":[],"source":["def plot_loss(train_loss_values, epochs, output_path):\n","  width = 460\n","  plt.style.use('seaborn-bright')\n","  tex_fonts = {\n","        # Use LaTeX to write all text\n","        \"text.usetex\": False,\n","        \"font.family\": \"serif\",\n","        # Use 10pt font in plots, to match 10pt font in document\n","        \"axes.labelsize\": 10,\n","        \"font.size\": 10,\n","        # Make the legend/label fonts a little smaller\n","        \"legend.fontsize\": 8,\n","        \"xtick.labelsize\": 8,\n","        \"ytick.labelsize\": 8\n","    }\n","  plt.rcParams.update(tex_fonts)\n","  epochs = np.arange(1, epochs + 1)\n","  train_loss_values = np.array(train_loss_values)\n","  plt.style.use('seaborn')\n","  fig, ax = plt.subplots(1, 1,figsize=set_size(width))  \n","  ax.plot(epochs, train_loss_values, 'b', label='Training Loss', linestyle='dotted')  \n","  ax.set_title('Training')\n","  ax.set_xlabel('Epochs')\n","  ax.set_ylabel('Loss')\n","  ax.legend()\n","  \n","  fig.savefig(output_path + '/loss.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()"]},{"cell_type":"markdown","metadata":{"id":"8bS18uCKr73p"},"source":["#### Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHOfN-8LsC4N"},"outputs":[],"source":["def plot_table(setting, param, dml_param, output_path):  \n","  width = 460\n","  plt.style.use('seaborn-bright')\n","  tex_fonts = {\n","        # Use LaTeX to write all text\n","        \"text.usetex\": False,\n","        \"font.family\": \"serif\",\n","        # Use 10pt font in plots, to match 10pt font in document\n","        \"axes.labelsize\": 10,\n","        \"font.size\": 10,\n","        # Make the legend/label fonts a little smaller\n","        \"legend.fontsize\": 8,\n","        \"xtick.labelsize\": 8,\n","        \"ytick.labelsize\": 8\n","    }\n","  plt.rcParams.update(tex_fonts)\n","\n","  ########## Plot Settings ##################\n","  setting_name_list = list(setting.keys())\n","  setting_value_list = list(setting.values())\n","  setting_name_list, setting_value_list = replace_helper(setting_name_list, setting_value_list)\n","  vals = np.array([setting_name_list, setting_value_list], dtype=str).T\n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.table(cellText=vals, colLabels=['Setting', 'Value'], loc='center', zorder=3, rowLoc='left', cellLoc='left')\n","  ax.set_title('Experiment Settings')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  fig.savefig(output_path + '/settings.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()\n","\n","  ########## Plot Params ##################\n","  param_name_list = param.keys()\n","  param_value_list = param.values()\n","  param_name_list, param_value_list = replace_helper(param_name_list, param_value_list)\n","  param_vals = np.array([list(param_name_list), list(param_value_list)], dtype=str).T\n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.table(cellText=param_vals, colLabels=['Hyperparameter', 'Value'], loc='center', zorder=3, rowLoc='left', cellLoc='left')\n","  ax.set_title('Hyperparaeters')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  fig.savefig(output_path + '/params.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()\n","\n","  ########## Plot DML Params ##################\n","  dml_param_name_list = dml_param.keys()\n","  dml_param_value_list = dml_param.values()\n","  dml_param_name_list, dml_param_value_list = replace_helper(dml_param_name_list, dml_param_value_list)\n","  dml_param_vals = np.array([list(dml_param_name_list), list(dml_param_value_list)], dtype=str).T  \n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.table(cellText=dml_param_vals, colLabels=['DML Hyperparameter', 'Value'], loc='center', zorder=3, rowLoc='left', cellLoc='left')\n","  ax.set_title('DML Hyperparameters')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  fig.savefig(output_path + '/dml_params.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()"]},{"cell_type":"markdown","metadata":{"id":"Qcb6zwrolo8t"},"source":["### Dataloading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YnS1KU5paWt0"},"outputs":[],"source":["def create_processed_info(path, debug=False):\n","  if debug:\n","    info_path = join(path, 'debug_processed_info.csv')\n","  else:\n","    info_path = join(path, 'processed_info.csv')\n","  if isfile(info_path):\n","    processed_frame = pd.read_csv(info_path, index_col=0, dtype={'fnames':str,'papyID':int,'posinfo':str, 'pixelCentimer':float}, header=0)    \n","  else:    \n","    fnames = [f for f in listdir(path) if isfile(join(path, f))]\n","    fnames = [ x for x in fnames if \".png\" in x ]\n","    fnames = [f.split('.',1)[0] for f in fnames]\n","    fnames_frame = pd.DataFrame(fnames,columns=['fnames'])\n","    fragmentID = pd.DataFrame([f.split('_',1)[0] for f in fnames], columns=['fragmentID'])\n","    fnames_raw = [f.split('_',1)[1] for f in fnames]\n","    processed_frame = pd.DataFrame(fnames_raw, columns=['fnames_raw'])\n","    \n","    processed_frame = pd.concat([processed_frame, fnames_frame], axis=1)\n","\n","    processed_frame = pd.concat([processed_frame, fragmentID], axis=1)\n","    processed_frame['papyID'] = processed_frame.fnames_raw.apply(lambda x: x.split('_',1)[0])\n","    processed_frame['posinfo'] = processed_frame.fnames_raw.apply(lambda x: ''.join(filter(str.isalpha, x)))\n","    processed_frame['pixelCentimer'] = processed_frame.fnames_raw.progress_apply(retrive_size_by_fname)\n","    processed_frame.to_csv(info_path)\n","     \n","  return processed_frame"]},{"cell_type":"markdown","metadata":{"id":"uY5dZOoTltrr"},"source":["### Logging"]},{"cell_type":"markdown","metadata":{"id":"vOXhjFJXs2kf"},"source":["#### Thesis Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QYpRjvPs53A"},"outputs":[],"source":["def set_size(width, fraction=1, subplots=(1, 1)):\n","    \"\"\"Set figure dimensions to avoid scaling in LaTeX.\n","\n","    Parameters\n","    ----------\n","    width: float or string\n","            Document width in points, or string of predined document type\n","    fraction: float, optional\n","            Fraction of the width which you wish the figure to occupy\n","    subplots: array-like, optional\n","            The number of rows and columns of subplots.\n","    Returns\n","    -------\n","    fig_dim: tuple\n","            Dimensions of figure in inches\n","    \"\"\"\n","    if width == 'thesis':\n","        width_pt = 426.79135\n","    elif width == 'beamer':\n","        width_pt = 307.28987\n","    else:\n","        width_pt = width\n","\n","    fig_width_pt = width_pt * fraction\n","    inches_per_pt = 1 / 72.27\n","    golden_ratio = (5**.5 - 1) / 2\n","    fig_width_in = fig_width_pt * inches_per_pt\n","    fig_height_in = fig_width_in * golden_ratio * (subplots[0] / subplots[1])\n","\n","    return (fig_width_in, fig_height_in)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tDxF2I3Ks9ly"},"outputs":[],"source":["def replace_helper(some_list_1, some_list_2):\n","  new_list_1 = []\n","  new_list_2 = []\n","\n","  for string_a, string_b in zip(some_list_1,some_list_2):     \n","    new_list_1.append(str(string_a).replace(\"_\", \" \"))\n","    new_list_2.append(str(string_b).replace(\"_\", \" \"))\n","\n","  return new_list_1, new_list_2"]},{"cell_type":"markdown","metadata":{"id":"R_R59u4Lq5Nu"},"source":["#### Dir-Management"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NUxKU5SAhJ31"},"outputs":[],"source":["def create_output_dir(name, experiment_name, x=1):\n","  \n","  while True:\n","        dir_name = (name + (str(x) + '_iteration_' if x is not 0 else '') + '_' + experiment_name).strip()\n","        if not os.path.exists(dir_name):\n","            os.mkdir(dir_name)            \n","\n","            return dir_name\n","        else:\n","            x = x + 1"]},{"cell_type":"markdown","metadata":{"id":"e1OgCU8mq-62"},"source":["#### Report-PDF"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_matDnHhrFry"},"outputs":[],"source":["def create_logging(setting, param, dml_param, train_loss_values, map_vals, random_map_vals, train_map, epochs, output_dir, model):\n","  plot_table(setting, param, dml_param, output_dir)\n","  \n","  gradient_visualization(model.named_parameters(), output_dir)\n","  plot_loss(train_loss_values, epochs, output_dir)\n","  plot_acc(map_vals, random_map_vals, train_map, epochs, output_dir)\n","  \n","\n","  pdfs = ['/loss.pdf', '/acc.pdf', '/params.pdf','/dml_params.pdf', '/settings.pdf', '/gradients.pdf']\n","  bookmarks = ['Loss', 'Accuracy', 'Hyperparameters','DML Hyperparameters', 'Seetings','Gradients']\n","\n","  merger = PdfFileMerger()\n","\n","  for i, pdf in enumerate(pdfs):\n","      merger.append(output_dir + pdf, bookmark=bookmarks[i])\n","  \n","  pdf = FPDF()   \n","  pdf.add_page() \n","  pdf.set_font(\"Helvetica\", size = 6)\n","  # open the text file in read mode\n","  f = open(\"log.txt\", \"r\")\n","  \n","  # insert the texts in pdf\n","  for x in f:\n","    pdf.cell(200, 6, txt = x, ln = 1, align = 'l')\n","\n","    # save the pdf with name .pdf\n","  pdf.output(\"log.pdf\")   \n","  merger.append(\"log.pdf\", bookmark='Log')\n","  merger.write(output_dir + \"/report.pdf\")\n","  merger.close()\n","  \n","  copyfile('log.txt', output_dir + '/log.txt')"]},{"cell_type":"markdown","metadata":{"id":"cU-t7XJ3mXAU"},"source":["## Initialize"]},{"cell_type":"markdown","metadata":{"id":"ID5sUPitmkPc"},"source":["### Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PiALxuAMmT0w"},"outputs":[],"source":["device = torch.device(\"cuda\")\n","model = Net().to(device)\n","config = toml.load('./gdrive/MyDrive/mt/conf/conf.toml')\n","setting = config.get('settings')\n","param = config.get('params')\n","dml_param = config.get('dml_params')"]},{"cell_type":"markdown","metadata":{"id":"QQ3TeHbsnWI5"},"source":["### Logging"]},{"cell_type":"markdown","metadata":{"id":"77jTfu2gvTZ6"},"source":["#### Create Dir"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o1fXnaFDnY69"},"outputs":[],"source":["output_dir = create_output_dir(setting['output'], setting['experiment_name'])"]},{"cell_type":"markdown","metadata":{"id":"f8F1dloEm0AH"},"source":["### Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dji5twA7FKSs"},"outputs":[],"source":["batch_size_train = param['batch_size_train']\n","batch_size_val = param['batch_size_val']\n","lr = param['lr']\n","num_epochs = param['num_epochs']"]},{"cell_type":"markdown","metadata":{"id":"dRXAyi5rFC5l"},"source":["#### Optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Azjxi-2PFXnC"},"outputs":[],"source":["if param['optimizer'] == 'Adam':\n","  optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","elif param['optimizer'] == 'SGD': \n","  optimizer =optim.SGD(model.parameters(), lr=lr)\n","\n","elif param['optimizer'] == 'AdamW': \n","  optimizer =optim.SGD(model.parameters(), lr=lr)\n","\n","else:\n","  logger.error(' Optimizer is not supported or you have not specified one.')\n","  raise ValueError() "]},{"cell_type":"markdown","metadata":{"id":"lhg1JP3pYN4o"},"source":["#### Model Architecture"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jy6guWa2YRIg"},"outputs":[],"source":["if param['archi'] == 'SimpleCNN':\n","  model = Net().to(device)\n","\n","elif param['archi'] == 'efficientnetB0':\n","  model = EfficientNet.from_name('efficientnet-b0').to(device)\n","\n","elif param['archi'] == 'efficientnetB7':\n","  model = EfficientNet.from_name('efficientnet-b7').to(device)\n","  model._fc  = torch.nn.Identity()\n","\n","elif param['archi'] == 'densenet201':\n","  model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet201', pretrained=False).to(device)\n","  model.classifier  = torch.nn.Identity()\n","  \n","elif param['archi'] == 'ResNet':\n","  model = models.resnet18(pretrained=True).to(device)"]},{"cell_type":"markdown","metadata":{"id":"jdFZdwt9TBeK"},"source":["#### Scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rnc_XBjmS6aR"},"outputs":[],"source":["scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[12], gamma=0.1)"]},{"cell_type":"markdown","metadata":{"id":"ilTnaXrWoiWh"},"source":["### PyTorch-Metric-Learning Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"63ck7WL2F88E"},"source":["#### Distance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ma2dhx2oF5FL"},"outputs":[],"source":["if  dml_param['distance'] == 'CosineSimilarity':   \n","  distance = distances.CosineSimilarity()\n","\n","elif  dml_param['distance'] == 'LpDistance':   \n","  distance = distances.LpDistance(normalize_embeddings=True, p=2, power=1)\n","  \n","else:\n","  logger.error(' Distance is not supported or you have not specified one.') \n","  raise ValueError()"]},{"cell_type":"markdown","metadata":{"id":"TxZfdQaNGgbR"},"source":["#### Reducer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIUXkz1TGoJ7"},"outputs":[],"source":["if  dml_param['reducer'] == 'ThresholdReducer':   \n","  reducer = reducers.ThresholdReducer(low=dml_param['ThresholdReducer_low'])\n","\n","elif  dml_param['reducer'] == 'AvgNonZeroReducer':\n","  reducer = reducers.AvgNonZeroReducer()\n","  \n","else:\n","  logger.error(f'Reducer is not supported or you have not specified one.')\n","  raise ValueError() "]},{"cell_type":"markdown","metadata":{"id":"A-7WsPMHG7Do"},"source":["#### Los Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9tXTS8KG4kb"},"outputs":[],"source":["if dml_param['loss_function'] == 'TripletMarginLoss': \n","  loss_func = losses.TripletMarginLoss(margin=dml_param['TripletMarginLoss_margin'], distance=distance, reducer=reducer)\n","\n","elif dml_param['loss_function'] == 'ContrastiveLoss':\n","  loss_func = losses.ContrastiveLoss(pos_margin=1, neg_margin=0)\n","\n","elif dml_param['loss_function'] == 'CircleLoss':\n","  loss_func = losses.CircleLoss(m=dml_param['m'], gamma=dml_param['gamma'], distance=distance, reducer=reducer)\n","  \n","else:\n","  logger.error('DML Loss is not supported or you have not specified one.')\n","  raise ValueError() "]},{"cell_type":"markdown","metadata":{"id":"dcPTAeI1IEYp"},"source":["#### Mining Function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRSP72ujHkPD"},"outputs":[],"source":["if dml_param['miner'] == 'TripletMarginMiner':\n","  mining_func = miners.TripletMarginMiner(\n","      margin=dml_param['TripletMarginMiner_margin'],\n","      distance=distance,\n","      type_of_triplets=dml_param['type_of_triplets']\n","      )\n","\n","else:    \n","  logger.error('DML Miner is not supported or you have not specified one.')\n","  raise ValueError() "]},{"cell_type":"markdown","metadata":{"id":"VmXXFzUWIRV-"},"source":["#### Accuracy Calculator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YcfW8LmwIM2l"},"outputs":[],"source":["accuracy_calculator = AccuracyCalculator(include=(dml_param['metric_1'],                                                  \n","                                                  dml_param['metric_2']),                                                   \n","                                         k=dml_param['precision_at_1_k'])"]},{"cell_type":"markdown","metadata":{"id":"W21eTCa9FNyV"},"source":["### Transformations"]},{"cell_type":"markdown","metadata":{"id":"PTtxowc7MKhQ"},"source":["#### Custom Transformation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9s_MFvJMJgn"},"outputs":[],"source":["class NCrop(object):\n","    \"\"\"Rescale the image in a sample to a given size.\n","\n","    Args:\n","        output_size (tuple or int): Desired output size. If tuple, output is\n","            matched to output_size. If int, smaller of image edges is matched\n","            to output_size keeping aspect ratio the same.\n","    \"\"\"\n","\n","    def __init__(self, output_size, n):        \n","      self.output_size = output_size\n","      self.n = n\n","\n","    def __call__(self, sample):\n","      out = extract_patches_2d(sample, self.output_size, max_patches=self.n)\n","      out = out.transpose((0,3,2,1))\n","      out = torch.tensor(out)\n","      return out "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"534J1LREmyoJ"},"outputs":[],"source":["if param['transform'] == \"TenCrop\":\n","  train_transform = transforms.Compose([\n","                                  transforms.TenCrop((param['crop_1'],param['crop_2'])),\n","                                  transforms.Lambda(lambda crops: torch.stack([transforms.PILToTensor()(crop) for crop in crops])),                                \n","                                  transforms.ConvertImageDtype(torch.float),\n","                                  transforms.Normalize((param['normalize_1'],param['normalize_2'],param['normalize_3']), (param['normalize_4'],param['normalize_5'],param['normalize_6']))]\n","  )\n","\n","\n","  val_transform = transforms.Compose([                                                                                                           \n","                                  transforms.TenCrop((param['crop_1'],param['crop_2'])),\n","                                  transforms.Lambda(lambda crops: torch.stack([transforms.PILToTensor()(crop) for crop in crops])),\n","                                  transforms.ConvertImageDtype(torch.float),\n","                                  transforms.Normalize((param['normalize_1'],param['normalize_2'],param['normalize_3']), (param['normalize_4'],param['normalize_5'],param['normalize_6']))]\n","  )\n","elif param['transform'] == \"Ncrop\":\n","  train_transform = transforms.Compose([\n","                                  NCrop((param['crop_1'],param['crop_2']),n=param['max_patches_train']),                                  \n","                                  transforms.ConvertImageDtype(torch.float),\n","                                  transforms.Normalize((param['normalize_1'],param['normalize_2'],param['normalize_3']), (param['normalize_4'],param['normalize_5'],param['normalize_6']))]\n","  )\n","\n","  val_transform = transforms.Compose([                                                                                                           \n","                                  NCrop((param['crop_1'],param['crop_2']),n=param['max_patches_train']),                                  \n","                                  transforms.ConvertImageDtype(torch.float),\n","                                  transforms.Normalize((param['normalize_1'],param['normalize_2'],param['normalize_3']), (param['normalize_4'],param['normalize_5'],param['normalize_6']))]\n","  )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WA_l7KoCrFr1"},"outputs":[],"source":["if True:\n","\n","  train_transform = transforms.Compose([\n","                                  NCrop((param['crop_1'],param['crop_2']),n=param['max_patches_train']),                                  \n","                                  transforms.ConvertImageDtype(torch.float),\n","                                  transforms.Normalize((param['normalize_1'],param['normalize_2'],param['normalize_3']), (param['normalize_4'],param['normalize_5'],param['normalize_6']))]\n","  )\n","\n","  val_transform = transforms.Compose([                                                                                                           \n","                                  NCrop((param['crop_1'],param['crop_2']),n=param['max_patches_train']),                                  \n","                                  transforms.ConvertImageDtype(torch.float),\n","                                  transforms.Normalize((param['normalize_1'],param['normalize_2'],param['normalize_3']), (param['normalize_4'],param['normalize_5'],param['normalize_6']))]\n","  )"]},{"cell_type":"markdown","metadata":{"id":"bvE-rvIeni4c"},"source":["\n","### Data"]},{"cell_type":"markdown","metadata":{"id":"Qy3ZfyMynqHb"},"source":["#### Helpers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WseW_TFXnexV"},"outputs":[],"source":["processed_frame_train = create_processed_info(setting['path_train'])\n","processed_frame_val = create_processed_info(setting['path_val'])"]},{"cell_type":"markdown","metadata":{"id":"NK8UQnHinuh4"},"source":["#### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E78ZPAQGnwer"},"outputs":[],"source":["train_dataset = FAUPapyrusCollectionDataset(setting['path_train'], processed_frame_train, train_transform)\n","val_dataset = FAUPapyrusCollectionDataset(setting['path_val'], processed_frame_val, val_transform)"]},{"cell_type":"markdown","metadata":{"id":"7WTaXLwhnzbx"},"source":["#### Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNF7eFWEn_Np"},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_train, shuffle=param[\"shuffle\"], drop_last=True, num_workers=4)\n","test_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size_val, drop_last=param[\"shuffle\"], num_workers=4)"]},{"cell_type":"markdown","metadata":{"id":"dYUkEM-6opYf"},"source":["#### Result Lists"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8AR7S8PdovzO"},"outputs":[],"source":["loss_vals = []\n","val_loss_vals = []\n","map_vals = []\n","random_map_vals = []\n","train_map_vals = []"]},{"cell_type":"markdown","metadata":{"id":"iAvfRjeXoxrF"},"source":["## Training"]},{"cell_type":"markdown","metadata":{"id":"gsDx-9w_vcd_"},"source":["### Log Hyperparameters"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55,"status":"ok","timestamp":1642625994566,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"},"user_tz":-60},"id":"k-HR0NAlve9y","outputId":"d1d8e16e-1b9c-4770-9a3e-a839d33f0e13"},"outputs":[{"output_type":"stream","name":"stderr","text":["Debug:                False\n","Loos Function:        TripletMarginLoss\n","Margin Miner Margin:  0.2\n","Triplet Margin Loss:  0.2\n","Type of Tribles:      semihard\n","Miner:                TripletMarginMiner\n","Reducer:              AvgNonZeroReducer\n","Archi:                efficientnetB7\n","Epochs:               20\n","Batch Size Train:     64\n","Batch Size Val:       1\n","Optimizer:            SGD\n","Learning Rate:        0.0001\n","Shuffle:              True\n"]}],"source":["logger.info(f'Debug:                {setting[\"debug\"]}')\n","logger.info(f'Loos Function:        {dml_param[\"loss_function\"]}')\n","logger.info(f'Margin Miner Margin:  {dml_param[\"TripletMarginMiner_margin\"]}')\n","logger.info(f'Triplet Margin Loss:  {dml_param[\"TripletMarginLoss_margin\"]}')\n","logger.info(f'Type of Tribles:      {dml_param[\"type_of_triplets\"]}')\n","logger.info(f'Miner:                {dml_param[\"miner\"]}')\n","logger.info(f'Reducer:              {dml_param[\"reducer\"]}')\n","logger.info(f'Archi:                {param[\"archi\"]}')\n","logger.info(f'Epochs:               {param[\"num_epochs\"]}')\n","logger.info(f'Batch Size Train:     {param[\"batch_size_train\"]}')\n","logger.info(f'Batch Size Val:       {param[\"batch_size_val\"]}')\n","logger.info(f'Optimizer:            {param[\"optimizer\"]}')\n","logger.info(f'Learning Rate:        {param[\"lr\"]}')\n","logger.info(f'Shuffle:              {param[\"shuffle\"]}')"]},{"cell_type":"markdown","metadata":{"id":"ry9CMFfQvqfa"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"id":"GJ_L0TrTDnEA","executionInfo":{"status":"error","timestamp":1642626057899,"user_tz":-60,"elapsed":63380,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"}},"outputId":"29808684-ce1a-4d0f-d783-21401ec91d92"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-8eec6fa3583f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m           \u001b[0maccuracy_calculator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m           \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m           \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accumulation_steps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m           )\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-9-0bf83a152661>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loss_func, mining_func, device, train_loader, optimizer, train_set, epoch, accuracy_calculator, scheduler, accumulation_steps)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;31m#optimizer.zero_grad()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_imgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0membeddings_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncrops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \"\"\"\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m# Convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Pooling and final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_avg_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mdrop_connect_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# scale drop connect_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depthwise_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/padding.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4172\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Padding length too large\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"constant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_pad_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4176\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding mode \"{}\"\" doesn\\'t take in value argument'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 226.00 MiB (GPU 0; 15.90 GiB total capacity; 14.60 GiB already allocated; 103.75 MiB free; 14.69 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["if setting[\"training\"]:\n","  old_map = 0\n","\n","  for epoch in range(1, num_epochs + 1):\n","      ############### Training ###############\n","      train_loss, train_map = train(\n","          model,\n","          loss_func,mining_func,\n","          device,\n","          train_loader,\n","          optimizer,\n","          train_dataset,\n","          epoch,  \n","          accuracy_calculator,\n","          scheduler,\n","          accumulation_steps=param[\"accumulation_steps\"]         \n","          )\n","      \n","\n","      ############### Validation ###############\n","      map, random_map = val(val_dataset, val_dataset, model, accuracy_calculator)\n","    \n","\n","      ############### Fill Lists ###############\n","      loss_vals.append(train_loss)\n","      map_vals.append(map)\n","      random_map_vals.append(random_map)\n","      train_map_vals.append(train_map)\n","      ############### Checkpoint ###############\n","      \n","      if map >= old_map: \n","        torch.save({\n","                    'epoch': epoch,\n","                    'model_state_dict': model.state_dict(),\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'loss': train_loss,\n","                    }, output_dir + \"/model.pt\")\n","      \n","      old_map = map\n","      ############### Logging ###############\n","      create_logging(setting, param, dml_param, loss_vals, map_vals, random_map_vals, train_map_vals, epoch, output_dir, model)"]},{"cell_type":"markdown","metadata":{"id":"JWMWRWk8km5r"},"source":["## Inference"]},{"cell_type":"markdown","metadata":{"id":"HJLQXPOs2elc"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCtt35U3kpvC"},"outputs":[],"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torchvision\n","from torchvision import datasets, transforms\n","import cv2\n","from pytorch_metric_learning.distances import CosineSimilarity\n","from pytorch_metric_learning.utils import common_functions as c_f\n","from pytorch_metric_learning.utils.inference import InferenceModel, MatchFinder\n","import json\n","import skimage"]},{"cell_type":"markdown","metadata":{"id":"FdkO3QwX2a4r"},"source":["### Helpers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZaeHmhlmkthQ"},"outputs":[],"source":["def print_decision(is_match):\n","    if is_match:\n","        print(\"Same class\")\n","    else:\n","        print(\"Different class\")\n","\n","\n","mean = [0.6143, 0.6884, 0.7665]\n","std = [0.229, 0.224, 0.225]\n","\n","inv_normalize = transforms.Normalize(\n","    mean=[-m / s for m, s in zip(mean, std)], std=[1 / s for s in std]\n",")\n","import numpy as np\n","import cv2\n","import json\n","from matplotlib import pyplot as plt\n","\n","\n","def imshow(img, figsize=(21, 9), boarder=None, get_img = False):\n","    img = inv_normalize(img)\n","    BLUE = [255,0,0]\n","    npimg = img.numpy()\n","    transposed = np.transpose(npimg, (1, 2, 0))\n","    #boarderized = draw_border(transposed, bt=5, with_plot=False, gray_scale=False, color_name=\"red\")\n","    x = int(transposed.shape[1] * 0.025)\n","    y = int(transposed.shape[2] * 0.025)\n","    if x > y:\n","      y=x\n","    else:\n","      y=x\n","\n","    if boarder == 'green':\n","      boarderized = cv2.copyMakeBorder(transposed,x,x,y,y,cv2.BORDER_CONSTANT,value=[0,255,0])\n","    elif boarder == 'red':\n","      boarderized = cv2.copyMakeBorder(transposed,x,x,y,y,cv2.BORDER_CONSTANT,value=[255,0,0])\n","    else:\n","      boarderized = transposed\n","    if get_img:\n","      return boarderized\n","    else:\n","      plt.figure(figsize=figsize)\n","      plt.imshow((boarderized * 255).astype(np.uint8))\n","      plt.show()"]},{"cell_type":"markdown","metadata":{"id":"oR31Ixjr2kyu"},"source":["### Transform"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9tlskYTlHFc"},"outputs":[],"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)]\n",")"]},{"cell_type":"markdown","metadata":{"id":"27BBiXUp2tLL"},"source":["### Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmzHsTrpkOSR"},"outputs":[],"source":["class FAUPapyrusCollectionInferenceDataset(torch.utils.data.Dataset):\n","    \"\"\"FAUPapyrusCollection dataset.\"\"\"\n","    def __init__(self, root_dir, processed_frame, transform=None):\n","\n","        self.root_dir = root_dir\n","        self.processed_frame = processed_frame\n","        self.transform = transform\n","        self.targets = processed_frame[\"papyID\"].unique()\n","\n","    def __len__(self):\n","        return len(self.processed_frame)       \n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.processed_frame.iloc[idx, 1])\n","        \n","        img_name = img_name + '.png'\n","        \n","        #image = io.imread(img_name , plugin='matploPILtlib')        \n","        image = PIL.Image.open(img_name)\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        #if False:\n","        max_img_size = 2048\n","\n","        if (image.shape[1] > max_img_size) or (image.shape[2] > max_img_size):\n","          image = transforms.CenterCrop(max_img_size)(image)          \n","\n","        papyID = self.processed_frame.iloc[idx,3]\n","\n","\n","\n","        return image, papyID"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxRzgXb__HfE"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hl8Wehw4_Mz_"},"outputs":[],"source":["# No longer needed deleated soon\n","class MyInferenceModel(InferenceModel):\n","  \n","    def get_embeddings_from_tensor_or_dataset(self, inputs, batch_size):\n","        inputs = self.process_if_list(inputs)\n","        embeddings = []\n","        if isinstance(inputs, (torch.Tensor, list)):\n","            for i in range(0, len(inputs), batch_size):\n","                embeddings.append(self.get_embeddings(inputs[i : i + batch_size]))\n","        elif isinstance(inputs, torch.utils.data.Dataset):\n","            dataloader = torch.utils.data.DataLoader(inputs, batch_size=batch_size)\n","            for inp, _ in dataloader:\n","                embeddings.append(self.get_embeddings(inp))\n","        else:\n","            raise TypeError(f\"Indexing {type(inputs)} is not supported.\")\n","        return torch.cat(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fKyVHjpo2x2v"},"outputs":[],"source":["dataset = FAUPapyrusCollectionInferenceDataset(setting['path_val'], processed_frame_val, transform)"]},{"cell_type":"markdown","metadata":{"id":"M1WfwwST20Vp"},"source":["### Apply DML-Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MXOuUSmUrJ9b"},"outputs":[],"source":["def get_labels_to_indices(dataset):\n","  labels_to_indices = {}\n","  for i, sample in enumerate(dataset):\n","    img, label = sample\n","    if label in labels_to_indices.keys():\n","      labels_to_indices[label].append(i)\n","    else:\n","      labels_to_indices[label] = [i]\n","  return labels_to_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vfVfPjUVsWTh"},"outputs":[],"source":["labels_to_indices = get_labels_to_indices(dataset)"]},{"cell_type":"markdown","metadata":{"id":"Hj7P-1ye26Cy"},"source":["### Load Checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANpqzd8gmJzk"},"outputs":[],"source":["model =  model = EfficientNet.from_name('efficientnet-b7').to(device)\n","model._fc  = torch.nn.Identity()\n","checkpoint = torch.load(output_dir + \"/model.pt\")\n","model.load_state_dict(checkpoint['model_state_dict'])\n","optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","epoch = checkpoint['epoch']\n","loss = checkpoint['loss']\n","model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"vDxYm6eM1U__"},"source":["### Prepare DML Methods"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DyeQpjWTrHrm"},"outputs":[],"source":["match_finder = MatchFinder(distance=CosineSimilarity(), threshold=0.2)\n","inference_model = InferenceModel(model, match_finder=match_finder)"]},{"cell_type":"markdown","metadata":{"id":"iSQgtXgX1fMS"},"source":["### PapyIDs to Index"]},{"cell_type":"markdown","metadata":{"id":"_-rb5-h21ngF"},"source":["### Prepare KNN for Inference on Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r4fNAV1SrJua"},"outputs":[],"source":["# create faiss index\n","inference_model.train_knn(dataset, batch_size=1)"]},{"cell_type":"markdown","metadata":{"id":"DSWyMed23IRn"},"source":["### Infercening"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"84b7gv3orNcC"},"outputs":[],"source":["k = 100\n","\n","lowest_acc = 1\n","highest_acc = 0\n","\n","temp_counter = 0\n","max_counter = 2\n","\n","\n","for papyID in labels_to_indices.keys():\n","  if temp_counter >=max_counter:\n","    break\n","  \n","  for fragment in labels_to_indices[papyID]:\n","    if temp_counter >=max_counter:\n","      break\n","    temp_counter = temp_counter + 1\n","    img, org_label = dataset[fragment]\n","    img = img.unsqueeze(0)\n","    #print(f\"query image: {org_label}\")\n","    #imshow(torchvision.utils.make_grid(img))\n","    distances, indices = inference_model.get_nearest_neighbors(img, k=k)\n","    #print(len(distances[0]))\n","    \n","    nearest_imgs = [dataset[i][0] for i in indices.cpu()[0]]\n","    #print(f\"Nearest Images:\\n\")\n","    \n","\n","    neighbours = []\n","    labels = []\n","    for i in indices.cpu()[0]:\n","      neighbour, label = dataset[i]\n","      \n","      #print(f\"Label: {label}\")\n","      neighbours.append(neighbour)\n","      labels.append(label)\n","    \n","    occurrences = labels.count(org_label)\n","    acc = occurrences / 100\n","\n","\n","    if acc < lowest_acc:\n","      lowest_acc = acc\n","      print(f'Found new lowest example with acc {acc}')\n","      input_img_of_lowest_acc = img\n","      input_label_of_lowest_acc = org_label\n","      input_index_of_lowest_acc = fragment\n","      detected_neighbours_of_lowest_acc = neighbours\n","      detected_labels_of_lowest_acc = labels\n","      detected_distances_of_lowest_acc = distances\n","\n","    if acc > highest_acc:\n","      highest_acc = acc\n","      print(f'Found new highest example with acc {acc}')\n","      input_img_of_highest_acc = img\n","      input_label_of_highest_acc = org_label\n","      input_index_of_highest_acc = fragment\n","      detected_neighbours_of_highest_acc = neighbours\n","      detected_labels_of_highest_acc = labels\n","      detected_distances_of_highest_acc = distances\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z-8wzPEmCRRb"},"outputs":[],"source":["  def get_inference_plot(neighbours, labels, distances, org_label, img ,k, lowest):\n","    if lowest:\n","      print(f\"query image for lowest acc: {org_label}\")\n","    else:      \n","      print(f\"query image for highest acc: {org_label}\")\n","\n","    imshow(torchvision.utils.make_grid(img))\n","\n","    Nr = k\n","    Nc = 10\n","    my_dpi = 96\n","    fig, axs = plt.subplots(Nr, Nc)\n","    fig.set_figheight(320)\n","    fig.set_figwidth(30)\n","    fig.suptitle(f'Neighbour Crops of {org_label}')\n","\n","    for i, neighbour in enumerate(neighbours):      \n","      #print(neighbour.shape)\n","      neighbour_crops = extract_patches_2d(image=neighbour.T.numpy(), patch_size=(32,32), max_patches= 10)\n","      neighbour_crops = neighbour_crops.transpose((0,3,2,1))\n","      neighbour_crops = torch.tensor(neighbour_crops)\n","      for j in range(Nc):                        \n","        if j == 0:\n","          distance = (distances[i].cpu().numpy().round(2))\n","\n","          row_label = f\"label: {labels[i]} \\n distance: {distance}\"\n","          axs[i,j].set_ylabel(row_label)\n","\n","        neighbour_crop = neighbour_crops[j]\n","        img = inv_normalize(neighbour_crop)\n","        npimg = img.numpy()\n","        transposed = np.transpose(npimg, (1, 2, 0))\n","        \n","        # find right size for the frame\n","        x = int(transposed.shape[1] * 0.05)\n","        \n","\n","        boarder = 'green'\n","\n","        if org_label == labels[i]:\n","          boarderized = cv2.copyMakeBorder(transposed,x,x,x,x,cv2.BORDER_CONSTANT,value=[0,1,0])\n","        elif org_label != labels[i]:\n","          boarderized = cv2.copyMakeBorder(transposed,x,x,x,x,cv2.BORDER_CONSTANT,value=[1,0,0])\n","        else:\n","          boarderized = transposed\n","\n","        axs[i,j].imshow(boarderized, aspect='auto')\n","        \n","    plt.tight_layout()\n","    if lowest:\n","      plt.savefig(output_dir + \"/results_for_lowest_acc.pdf\",bbox_inches='tight',dpi=100)\n","    else:\n","      plt.savefig(output_dir + \"/results_for_highest_acc.pdf\",bbox_inches='tight',dpi=100)\n","    plt.show()\n","\n","  #get_inference_plot(neighbours, labels, distances[0], org_label, img, k=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aRF_3YxlC90P"},"outputs":[],"source":["get_inference_plot(detected_neighbours_of_highest_acc, detected_labels_of_highest_acc, detected_distances_of_highest_acc[0], input_label_of_highest_acc, input_img_of_highest_acc, k=100, lowest=False)\n","get_inference_plot(detected_neighbours_of_lowest_acc, detected_labels_of_lowest_acc, detected_distances_of_lowest_acc[0], input_label_of_lowest_acc, input_img_of_lowest_acc, k=100, lowest=True)      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tr0Y6fBhGvga"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"0.2.7-tb-train.ipynb","provenance":[{"file_id":"1Z2JtGH4D5G0mJ-E0mQDajoMGMAbXoC57","timestamp":1642426777906},{"file_id":"1izhDpKHanssbTmxUu7EVRYCOxB8f5PIs","timestamp":1641988891616},{"file_id":"1gxDBBsWnotCl1C00l-vxh5E7mcmkZ4wT","timestamp":1640081619510},{"file_id":"10u_u0_osI6y5JviWcnWhh9ihCFP-4JX8","timestamp":1639748676495},{"file_id":"13yQSmVzddAa7-qxOEUtEX3Tt89I6ETpV","timestamp":1639677526935},{"file_id":"https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/TripletMarginLossMNIST.ipynb","timestamp":1639668015260}],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}