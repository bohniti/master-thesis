{"cells":[{"cell_type":"markdown","metadata":{"id":"XBSQNXlCPwcq"},"source":["# Deep Metric Learning with FAU's Papyrus Collection\n","\n","## Customize Notebook and Install Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wp38kg6_jBpv","executionInfo":{"status":"ok","timestamp":1639662981951,"user_tz":-60,"elapsed":8389,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"}},"outputId":"eedba34d-e02c-4ced-dc3f-982c69bf4e3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: faiss-gpu in /usr/local/lib/python3.7/dist-packages (1.7.1.post3)\n","Requirement already satisfied: pytorch-metric-learning in /usr/local/lib/python3.7/dist-packages (1.0.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (0.11.1+cu111)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.10.0+cu111)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.0.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-metric-learning) (4.62.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->pytorch-metric-learning) (3.10.0.2)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pytorch-metric-learning) (3.0.0)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch-metric-learning) (7.1.2)\n","Requirement already satisfied: efficientnet_pytorch in /usr/local/lib/python3.7/dist-packages (0.7.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet_pytorch) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet_pytorch) (3.10.0.2)\n"]}],"source":["!pip install faiss-gpu\n","!pip install pytorch-metric-learning\n","!pip install efficientnet_pytorch    <"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":277,"referenced_widgets":["b0e097be3bcf4a92b2f1506f7d3c47af","f73fbafeb8c54eb68d8e5692a16fcca6","85febc5e968443509ba3731e0b2da29d","7a5dea9d8c364977a298b61f9d5c9f28","7aa2051d96da4ebbb2a30ef53415b11d","3d89c5297c9b4dfeaae4df9f12965010","18c945feefda466e826e400fba7eec02","54a2198b800b4bb8aba3c43329574a08","5883a7227b8342588c5abf0b2a753b8e","596892744ef04c3ea227e04f9c1062fa","df7a6a90b7ad4ef09710133ea17324c4"]},"id":"j8Fs4Tnkg-iG","executionInfo":{"status":"ok","timestamp":1639663473130,"user_tz":-60,"elapsed":6695,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"}},"outputId":"2b9b4d45-c7fe-4a39-874e-35903cb4f278"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.7/dist-packages (1.26.0)\n","Requirement already satisfied: FPDF in /usr/local/lib/python3.7/dist-packages (1.7.2)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n","Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0e097be3bcf4a92b2f1506f7d3c47af","version_minor":0,"version_major":2},"text/plain":["0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","cm-super is already the newest version (0.3.4-11).\n","dvipng is already the newest version (1.15-1).\n","texlive-fonts-extra is already the newest version (2017.20180305-2).\n","texlive-fonts-recommended is already the newest version (2017.20180305-1).\n","0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"]}],"source":["!pip install PyPDF2\n","!pip install FPDF\n","\n","# For Timo's code\n","from tqdm import tqdm_notebook as tqdm\n","from skimage import io, transform\n","from PyPDF2 import PdfFileMerger\n","from shutil import copyfile\n","from os.path import isfile, join\n","import matplotlib.pyplot as plt\n","from google.colab import drive\n","\n","from os import listdir\n","from fpdf import FPDF\n","import pandas as pd\n","import numpy as np\n","import PIL\n","import time\n","import toml\n","import cv2\n","import os\n","\n","# For custom dataset\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils, models\n","\n","# From Kevin Musgraves GitHub\n","from efficientnet_pytorch import EfficientNet\n","from pytorch_metric_learning import distances, losses, miners, reducers, testers, samplers\n","from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch\n","\n","# For Logging\n","from scipy.interpolate import make_interp_spline\n","\n","# Notebook Seetings\n","tqdm().pandas()\n","import matplotlib\n","from matplotlib import rc\n","rc('text', usetex=True)\n","matplotlib.rcParams['text.latex.preamble'] = [r'\\usepackage{amsmath}']\n","!apt install texlive-fonts-recommended texlive-fonts-extra cm-super dvipng\n","\n","\n","\n","!rm \"log.txt\"\n","import logging\n","#logging.basicConfig(filename=\"test.log\", level=logging.INFO )\n","logger = logging.getLogger('log')\n","logger.setLevel(logging.DEBUG)\n","# create file handler which logs even debug messages\n","fh = logging.FileHandler('log.txt')\n","fh.setLevel(logging.INFO)\n","# create console handler with a higher log level\n","ch = logging.StreamHandler()\n","ch.setLevel(logging.INFO)\n","# create formatter and add it to the handlers\n","formatter = logging.Formatter('%(message)s')\n","ch.setFormatter(formatter)\n","fh.setFormatter(formatter)\n","# add the handlers to logger\n","logger.addHandler(ch)\n","logger.addHandler(fh)\n","logger.propagate = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6qYnD02Oa3e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639662993067,"user_tz":-60,"elapsed":1697,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"}},"outputId":"828a12a9-2201-40fc-c891-c324637e7359"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NrFeUwTyFoIy"},"outputs":[],"source":["if False:\n","  info_path = './gdrive/MyDrive/mt/data/06_test/processed_info.csv'\n","  info_frame = pd.read_csv(info_path, index_col=0, dtype={'fnames_raw':str,'fnames':str,'fragmentID':int,'papyID':int,'posinfo':str, 'pixelCentimeter':float, 'Simpleposinfo':str,'papyPosID':str}, header=0)\n","  info_frame.papyPosID = info_frame.papyPosID.str.replace('v','0')\n","  info_frame.papyPosID = info_frame.papyPosID.str.replace('r','1')\n","  info_frame.papyPosID = info_frame.papyPosID.astype(int)\n","  info_frame.to_csv(info_path)"]},{"cell_type":"markdown","metadata":{"id":"jv9abKRwocm-"},"source":["## Prepare Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1FDOnr5SVM_g"},"outputs":[],"source":["def get_info(path):\n","    info_path = join(path, 'info.csv')\n","    if isfile(info_path): \n","        info_frame = pd.read_csv(info_path, index_col=0, dtype={'fnames_raw':str,\n","                                                                'fnames':str,\n","                                                                'fragmentID':int,\n","                                                                'papyID':int,\n","                                                                'posinfo':str,\n","                                                                'pixelCentimeter':float,\n","                                                                'Simpleposinfo':str,\n","                                                                'papyPosID':int}, header=0)\n","    else:\n","        fnames = [f for f in listdir(path) if isfile(join(path, f))]\n","        fnames = [ x for x in fnames if \".jpg\" in x ]\n","        fnames = [f.split('.',1)[0] for f in fnames]\n","        info_frame = pd.DataFrame(fnames, columns=['fnames'])\n","        info_frame['papyID'] = info_frame.fnames.apply(lambda x: x.split('_',1)[0])\n","        info_frame['posinfo'] = info_frame.fnames.apply(lambda x: ''.join(filter(str.isalpha, x)))\n","        info_path = join(path, 'info.csv')\n","        info_frame['pixelCentimer'] = info_frame.fnames.progress_apply(get_estimated_resulution)\n","        split_info_frame = pd.DataFrame(info_frame['pixelCentimer'].tolist(), columns=['pixelCM_Y','pixelCM_X'])\n","        info_frame = pd.concat([info_frame, split_info_frame], axis=1)\n","        info_frame.drop('pixelCentimer', axis=1, inplace=True)       \n","        info_frame['pixelCM'] = info_frame[['pixelCM_Y','pixelCM_X']].max(axis=1)\n","        info_frame.drop(columns=['pixelCM_X','pixelCM_Y'], inplace=True)\n","        info_frame.to_csv(info_path)        \n","        time.sleep(10)\n","    return info_frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAdZAEktTZOa"},"outputs":[],"source":["def retrive_size_by_fname(fname):\n","  path = './gdrive/MyDrive/mt/data/'\n","  info_frame = get_info(path=path)\n","  return float(info_frame.loc[info_frame['fnames'] == fname]['pixelCM'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VtBruOj_PSEt"},"outputs":[],"source":["def create_processed_info(path, debug=False):\n","  if debug:\n","    info_path = join(path, 'debug_processed_info.csv')\n","  else:\n","    info_path = join(path, 'processed_info.csv')\n","  if isfile(info_path):\n","    processed_frame = pd.read_csv(info_path, index_col=0, dtype={'fnames':str,'papyID':int,'posinfo':str, 'pixelCentimer':float}, header=0)    \n","  else:    \n","    fnames = [f for f in listdir(path) if isfile(join(path, f))]\n","    fnames = [ x for x in fnames if \".png\" in x ]\n","    fnames = [f.split('.',1)[0] for f in fnames]\n","    fnames_frame = pd.DataFrame(fnames,columns=['fnames'])\n","    fragmentID = pd.DataFrame([f.split('_',1)[0] for f in fnames], columns=['fragmentID'])\n","    fnames_raw = [f.split('_',1)[1] for f in fnames]\n","    processed_frame = pd.DataFrame(fnames_raw, columns=['fnames_raw'])\n","    \n","    processed_frame = pd.concat([processed_frame, fnames_frame], axis=1)\n","\n","    processed_frame = pd.concat([processed_frame, fragmentID], axis=1)\n","    processed_frame['papyID'] = processed_frame.fnames_raw.apply(lambda x: x.split('_',1)[0])\n","    processed_frame['posinfo'] = processed_frame.fnames_raw.apply(lambda x: ''.join(filter(str.isalpha, x)))\n","    processed_frame['pixelCentimer'] = processed_frame.fnames_raw.progress_apply(retrive_size_by_fname)\n","    processed_frame.to_csv(info_path)\n","     \n","  return processed_frame"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bMzeNDg00Zor"},"outputs":[],"source":["class FAUPapyrusCollectionDataset(Dataset):\n","    \"\"\"FAUPapyrusCollection dataset.\"\"\"\n","    def __init__(self, root_dir, processed_frame, transform=None):\n","\n","        self.root_dir = root_dir\n","        self.processed_frame = processed_frame\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.processed_frame)       \n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","\n","        img_name = os.path.join(self.root_dir,\n","                                self.processed_frame.iloc[idx, 1])\n","        \n","        img_name = img_name + '.png'\n","        \n","        #image = io.imread(img_name , plugin='matploPILtlib')        \n","        image = PIL.Image.open(img_name)\n","        if self.transform:\n","            image = self.transform(image)         \n","\n","        papyID = self.processed_frame.iloc[idx,3]\n","\n","\n","        return image, papyID\n","        #sample"]},{"cell_type":"markdown","metadata":{"id":"IF1UjH07otLd"},"source":["## Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZDjTXSCJa5B"},"outputs":[],"source":["### convenient function from pytorch-metric-learning ###\n","def get_all_embeddings(dataset, model):\n","    tester = testers.BaseTester()\n","    return tester.get_all_embeddings(dataset, model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z1Gu2ISvz01O"},"outputs":[],"source":["def create_output_dir(name, experiment_name, x=1):\n","  \n","  while True:\n","        dir_name = (name + (str(x) + '_iteration_' if x is not 0 else '') + 'of_experiment_' + experiment_name).strip()\n","        if not os.path.exists(dir_name):\n","            os.mkdir(dir_name)            \n","\n","            return dir_name\n","        else:\n","            x = x + 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MPw2K6TuWkk1"},"outputs":[],"source":["def replace_helper(some_list_1, some_list_2):\n","  new_list_1 = []\n","  new_list_2 = []\n","\n","  for string_a, string_b in zip(some_list_1,some_list_2):     \n","    new_list_1.append(str(string_a).replace(\"_\", \" \"))\n","    new_list_2.append(str(string_b).replace(\"_\", \" \"))\n","\n","  return new_list_1, new_list_2"]},{"cell_type":"markdown","metadata":{"id":"pT3qQRMF_LW5"},"source":["## Model Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cUkOVLPbJSaY"},"outputs":[],"source":["### MNIST code originally from https://github.com/pytorch/examples/blob/master/mnist/main.py ###\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, 3, 1)\n","        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n","        self.dropout1 = nn.Dropout2d(0.25)\n","        self.dropout2 = nn.Dropout2d(0.5)\n","        self.fc1 = nn.Linear(12544, 128)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = F.max_pool2d(x, 2)\n","        x = self.dropout1(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"9HWRt0P-o1HM"},"source":["## Define Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2knRkTTfJUnp"},"outputs":[],"source":["def train(model, loss_func, mining_func, device, train_loader, optimizer, epoch):\n","    model.train()\n","    \n","    for batch_idx, (data, labels) in enumerate(train_loader):\n","        data, labels = data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","\n","        embeddings = model(data)\n","\n","        indices_tuple = mining_func(embeddings, labels)\n","\n","        loss = loss_func(embeddings, labels, indices_tuple)        \n","        \n","        loss.backward()        \n","        optimizer.step()\n","\n","        # Console-Logging\n","        if batch_idx % 20 == 0:\n","          logger.info(f' Training:')\n","          logger.info(f'  Mined Tiplets {mining_func.num_triplets}')\n","          logger.info(f'  Loss {loss}')\n","            \n","    return loss, mining_func.num_triplets"]},{"cell_type":"markdown","metadata":{"id":"j80M-khQo4ql"},"source":["## Define Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2O00Kt7k2vf"},"outputs":[],"source":["def validation(model, loss_func, mining_func, device, eval_loader, optimizer, train_set, eval_set, accuracy_calculator,epoch):\n","    model.eval()\n","    \n","    for batch_idx, (data, labels) in enumerate(eval_loader):\n","        data, labels = data.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        embeddings = model(data)\n","        indices_tuple = mining_func(embeddings, labels)\n","        loss = loss_func(embeddings, labels, indices_tuple)              \n","\n","        # Console-Logging\n","        if batch_idx % 20 == 0:\n","          logger.info(f' Validation:')\n","          logger.info(f'  Mined Tiplets: {mining_func.num_triplets}')\n","          logger.info(f'  Loss{loss}')\n","                  \n","    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n","    eval_embeddings, eval_labels = get_all_embeddings(eval_set, model)\n","\n","    train_labels = train_labels.squeeze(1)\n","    eval_labels = eval_labels.squeeze(1)\n","\n","    accuracies = accuracy_calculator.get_accuracy(\n","        eval_embeddings, train_embeddings, eval_labels, train_labels, False\n","    )  \n","    logger.info(f'  AMI {accuracies[\"AMI\"]}')\n","    logger.info(f'  NMI {accuracies[\"NMI\"]}')\n","    logger.info(f'  MAP {accuracies[\"mean_average_precision\"]}')\n","    logger.info(f'  P@1 {accuracies[\"precision_at_1\"]}')\n","    \n","\n","    return loss, mining_func.num_triplets, accuracies[\"AMI\"], accuracies[\"NMI\"], accuracies[\"mean_average_precision\"], accuracies[\"precision_at_1\"]"]},{"cell_type":"markdown","metadata":{"id":"3iWTJ6Lko798"},"source":["## Define Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0W0x0EyWJeD_"},"outputs":[],"source":["### compute accuracy using AccuracyCalculator from pytorch-metric-learning ###\n","def test(train_set, test_set, model, accuracy_calculator, epoch):\n","\n","    train_embeddings, train_labels = get_all_embeddings(train_set, model)\n","    test_embeddings, test_labels = get_all_embeddings(test_set, model)\n","\n","    train_labels = train_labels.squeeze(1)\n","    test_labels = test_labels.squeeze(1)\n","    accuracies = accuracy_calculator.get_accuracy(\n","        test_embeddings, train_embeddings, test_labels, train_labels, False\n","    )  \n","\n","    # Console-Logging\n","    logger.info(f' Test:')\n","    logger.info(f'  AMI {accuracies[\"AMI\"]}')\n","    logger.info(f'  NMI {accuracies[\"NMI\"]}')\n","    logger.info(f'  MAP {accuracies[\"mean_average_precision\"]}')\n","    logger.info(f'  P@1 {accuracies[\"precision_at_1\"]}')\n","\n","    return accuracies[\"AMI\"], accuracies[\"NMI\"], accuracies[\"mean_average_precision\"], accuracies[\"precision_at_1\"]"]},{"cell_type":"markdown","metadata":{"id":"zLhqp0iZpHBk"},"source":["## Visualizatin and Logging"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7XH1xlYIwJlJ"},"outputs":[],"source":["def set_size(width, fraction=1, subplots=(1, 1)):\n","    \"\"\"Set figure dimensions to avoid scaling in LaTeX.\n","\n","    Parameters\n","    ----------\n","    width: float or string\n","            Document width in points, or string of predined document type\n","    fraction: float, optional\n","            Fraction of the width which you wish the figure to occupy\n","    subplots: array-like, optional\n","            The number of rows and columns of subplots.\n","    Returns\n","    -------\n","    fig_dim: tuple\n","            Dimensions of figure in inches\n","    \"\"\"\n","    if width == 'thesis':\n","        width_pt = 426.79135\n","    elif width == 'beamer':\n","        width_pt = 307.28987\n","    else:\n","        width_pt = width\n","\n","    fig_width_pt = width_pt * fraction\n","    inches_per_pt = 1 / 72.27\n","    golden_ratio = (5**.5 - 1) / 2\n","    fig_width_in = fig_width_pt * inches_per_pt\n","    fig_height_in = fig_width_in * golden_ratio * (subplots[0] / subplots[1])\n","\n","    return (fig_width_in, fig_height_in)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Duo-lA9pB21"},"outputs":[],"source":["def plot_loss(train_loss_values, val_loss_values, epochs, output_path):  \n","  \n","  epochs = np.arange(1, epochs + 1)\n","  train_loss_values = np.array(train_loss_values)\n","  val_loss_values = np.array(val_loss_values)\n","  plt.style.use('seaborn')\n","  width = 460\n","  \n","  if True:\n","    tex_fonts = {\n","        # Use LaTeX to write all text\n","        \"text.usetex\": True,\n","        \"font.family\": \"serif\",\n","        # Use 10pt font in plots, to match 10pt font in document\n","        \"axes.labelsize\": 10,\n","        \"font.size\": 10,\n","        # Make the legend/label fonts a little smaller\n","        \"legend.fontsize\": 8,\n","        \"xtick.labelsize\": 8,\n","        \"ytick.labelsize\": 8,\n","        \"legend.loc\":'lower left'\n","    }\n","\n","    plt.rcParams.update(tex_fonts)\n","  \n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","\n","  # plot original lines\n","  ax.plot(epochs, train_loss_values, 'b', label='Training Loss', linestyle='dotted')\n","  ax.plot(epochs, val_loss_values, 'g', label='Validation Loss', linestyle='dotted')\n","\n","  # plot smoothed lines\n","  epochs_smooth = np.linspace(epochs.min(), epochs.max(), 300)\n","  a_BSpline_train = make_interp_spline(epochs, train_loss_values)\n","  a_BSpline_val = make_interp_spline(epochs, val_loss_values)\n","\n","  train_loss_smooth =  a_BSpline_train(epochs_smooth)\n","  val_loss_smooth = a_BSpline_val(epochs_smooth)\n","  ax.plot(epochs_smooth, train_loss_smooth, 'b', label='Training Smoothed Loss')\n","  ax.plot(epochs_smooth, val_loss_smooth, 'g', label='Validation Smoothed Loss')\n","\n","  \n","  ax.set_title('Training and Validation Loss')\n","  ax.set_xlabel('Epochs')\n","  ax.set_ylabel('Loss')\n","  ax.legend()\n","  # Save and remove excess whitespace\n","  fig.savefig(output_path + '/loss.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7eZzEpPwbdML"},"outputs":[],"source":["def plot_acc(val_AMI_values, val_NMI_values,val_mean_average_precision_values, val_precision_at_1_values, epochs, output_path):  \n","  epochs = np.arange(1, epochs + 1)\n","  plt.style.use('seaborn')\n","  width = 460\n","  tex_fonts = {\n","      # Use LaTeX to write all text\n","      \"text.usetex\": True,\n","      \"font.family\": \"serif\",\n","      # Use 10pt font in plots, to match 10pt font in document\n","      \"axes.labelsize\": 10,\n","      \"font.size\": 10,\n","      # Make the legend/label fonts a little smaller\n","      \"legend.fontsize\": 8,\n","      \"xtick.labelsize\": 8,\n","      \"ytick.labelsize\": 8,\n","      \"legend.loc\":'lower left'\n","  }\n","  plt.rcParams.update(tex_fonts)\n","  \n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.plot(epochs, val_AMI_values, 'b', label=' Val AMI', linestyle='dotted', linewidth=.3)\n","  ax.plot(epochs, val_NMI_values, 'g', label='Val NMI', linestyle='dotted', linewidth=.3)\n","  ax.plot(epochs, val_mean_average_precision_values, 'r', label='Val MAP', linestyle='dotted', linewidth=.3)\n","  ax.plot(epochs, val_precision_at_1_values, 'm', label='Val P@1', linestyle='dotted', linewidth=.3)\n","\n","  epochs_smooth = np.linspace(epochs.min(), epochs.max(), 300)\n","  a_BSpline_AMI = make_interp_spline(epochs, val_AMI_values)\n","  a_BSpline_NMI = make_interp_spline(epochs, val_NMI_values)\n","  a_BSpline_mean_average_precision = make_interp_spline(epochs, val_mean_average_precision_values)\n","  a_BSpline_precision_at_1 = make_interp_spline(epochs, val_precision_at_1_values)\n","  val_AMI_values_smooth =  a_BSpline_AMI(epochs_smooth)\n","  val_NMI_values_smooth = a_BSpline_NMI(epochs_smooth)\n","  train_loss_smooth_smooth =  a_BSpline_mean_average_precision(epochs_smooth)\n","  val_loss_smooth_smooth = a_BSpline_precision_at_1(epochs_smooth)\n","\n","  ax.plot(epochs_smooth, val_AMI_values_smooth, 'b', label='Val AMI Smoothed', linewidth=.6)\n","  ax.plot(epochs_smooth, val_NMI_values_smooth, 'g', label='Val NMI Smoothed', linewidth=.6)\n","  ax.plot(epochs_smooth, train_loss_smooth_smooth, 'r', label='Val MAP Smoothed', linewidth=.6)\n","  ax.plot(epochs_smooth, val_loss_smooth_smooth, 'm', label='Val P@1 Smoothed ', linewidth=.6)  \n","  ax.set_title('Validation Accuracy')\n","  ax.set_xlabel('Epochs')\n","  ax.set_ylabel('Accuracy')\n","  ax.legend()\n","  fig.savefig(output_path + '/acc.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zS4wjydX5sbN"},"outputs":[],"source":["def plot_table(setting, param, dml_param, output_path):  \n","  width = 460\n","  plt.style.use('seaborn-bright')\n","  tex_fonts = {\n","        # Use LaTeX to write all text\n","        \"text.usetex\": True,\n","        \"font.family\": \"serif\",\n","        # Use 10pt font in plots, to match 10pt font in document\n","        \"axes.labelsize\": 10,\n","        \"font.size\": 10,\n","        # Make the legend/label fonts a little smaller\n","        \"legend.fontsize\": 8,\n","        \"xtick.labelsize\": 8,\n","        \"ytick.labelsize\": 8\n","    }\n","  plt.rcParams.update(tex_fonts)\n","\n","  ########## Plot Settings ##################\n","  setting_name_list = list(setting.keys())\n","  setting_value_list = list(setting.values())\n","  setting_name_list, setting_value_list = replace_helper(setting_name_list, setting_value_list)\n","  vals = np.array([setting_name_list, setting_value_list], dtype=str).T\n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.table(cellText=vals, colLabels=['Setting', 'Value'], loc='center', zorder=3, rowLoc='left', cellLoc='left')\n","  ax.set_title('Experiment Settings')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  fig.savefig(output_path + '/settings.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()\n","\n","  ########## Plot Params ##################\n","  param_name_list = param.keys()\n","  param_value_list = param.values()\n","  param_name_list, param_value_list = replace_helper(param_name_list, param_value_list)\n","  param_vals = np.array([list(param_name_list), list(param_value_list)], dtype=str).T\n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.table(cellText=param_vals, colLabels=['Hyperparameter', 'Value'], loc='center', zorder=3, rowLoc='left', cellLoc='left')\n","  ax.set_title('Hyperparaeters')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  fig.savefig(output_path + '/params.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()\n","\n","  ########## Plot DML Params ##################\n","  dml_param_name_list = dml_param.keys()\n","  dml_param_value_list = dml_param.values()\n","  dml_param_name_list, dml_param_value_list = replace_helper(dml_param_name_list, dml_param_value_list)\n","  dml_param_vals = np.array([list(dml_param_name_list), list(dml_param_value_list)], dtype=str).T  \n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","  ax.table(cellText=dml_param_vals, colLabels=['DML Hyperparameter', 'Value'], loc='center', zorder=3, rowLoc='left', cellLoc='left')\n","  ax.set_title('DML Hyperparameters')\n","  ax.set_xticks([])\n","  ax.set_yticks([])\n","  fig.savefig(output_path + '/dml_params.pdf', format='pdf', bbox_inches='tight')\n","  plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuxKJ_oNVQDF"},"outputs":[],"source":["def gradient_visualization(parameters, results_folder: str):\n","    \"\"\"\n","    Returns the parameter gradients over the epoch.\n","    :param parameters: parameters of the network\n","    :type parameters: iterator\n","    :param results_folder: path to results folder\n","    :type results_folder: str\n","    \"\"\"\n","    tex_fonts = {\n","    # Use LaTeX to write all text\n","    \"text.usetex\": False,\n","    \"font.family\": \"serif\",\n","    # Use 10pt font in plots, to match 10pt font in document\n","    \"axes.labelsize\": 10,\n","    \"font.size\": 10,\n","    # Make the legend/label fonts a little smaller\n","    \"legend.fontsize\": 8,\n","    \"xtick.labelsize\": 8,\n","    \"ytick.labelsize\": 8,\n","    \"legend.loc\":'lower left'\n","}\n","\n","    plt.rcParams.update(tex_fonts)\n","\n","    ave_grads = []\n","    layers = []\n","\n","\n","    for n, p in parameters:\n","        if (p.requires_grad) and (\"bias\" not in n):\n","            layers.append(n)\n","            ave_grads.append(p.grad.abs().mean())\n","    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n","    plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color=\"k\")\n","    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n","    plt.xlim(xmin=0, xmax=len(ave_grads))\n","    plt.ylim(ymin=0, ymax=0.0075)\n","    plt.xlabel(\"Layers\")\n","    plt.ylabel(\"average gradient\")\n","    plt.title(\"Gradient Visualization\")\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.savefig(results_folder + \"/gradients.pdf\")\n","    plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGzvOVL0iZmu"},"outputs":[],"source":["def create_logging(setting, param, dml_param, train_loss_values, val_loss_values, val_AMI_values, val_NMI_values,val_mean_average_precision_values, val_precision_at_1_values, epochs, output_dir):\n","  plot_table(setting, param, dml_param, output_dir)\n","  \n","  plot_loss(train_loss_values, val_loss_values, epochs, output_dir)\n","  plot_acc(val_AMI_values, val_NMI_values,val_mean_average_precision_values, val_precision_at_1_values, epochs, output_dir)\n","  \n","\n","  pdfs = ['/loss.pdf', '/acc.pdf', '/params.pdf','/dml_params.pdf', '/settings.pdf','/HistogramFragAfterTrain.pdf','/HistogramFragAfterVal.pdf', '/HistogramFragTest.pdf', '/gradients.pdf']\n","  bookmarks = ['Loss', 'Accuracy', 'Hyperparameters','DML Hyperparameters', 'Seetings', 'HistogramTrain', 'HistrogramVal', 'HistrogramTest','Gradients']\n","\n","  merger = PdfFileMerger()\n","\n","  for i, pdf in enumerate(pdfs):\n","      merger.append(output_dir + pdf, bookmark=bookmarks[i])\n","  \n","  pdf = FPDF()   \n","  pdf.add_page() \n","  pdf.set_font(\"Helvetica\", size = 6)\n","  # open the text file in read mode\n","  f = open(\"log.txt\", \"r\")\n","  \n","  # insert the texts in pdf\n","  for x in f:\n","    pdf.cell(200, 6, txt = x, ln = 1, align = 'l')\n","\n","    # save the pdf with name .pdf\n","  pdf.output(\"log.pdf\")   \n","  merger.append(\"log.pdf\", bookmark='Log')\n","  merger.write(output_dir + \"/report.pdf\")\n","  merger.close()\n","  \n","  copyfile('log.txt', output_dir + '/log.txt')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzoPDE80aqPN"},"outputs":[],"source":["def remove_empty_bins(counts):\n","  ticks = range(len(counts))\n","  new_ticks = []\n","  new_counts = []\n","  for tick, count in zip(ticks, counts):\n","    if count != 0:\n","      new_ticks.append(str(tick))\n","      new_counts.append(count)\n","  return new_ticks, new_counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l0HK2LpKPPft"},"outputs":[],"source":["def get_cleand_papyri_hist(processed_frame, title, fig_name):\n","  id_series = processed_frame.groupby('papyPosID')['fnames'].nunique().sort_values(ascending=False)\n","  counts = np.bincount(id_series)  \n","  ticks, counts = remove_empty_bins(counts)\n","  plt.style.use('seaborn')\n","  width = 460\n","\n","\n","  tex_fonts = {\n","      # Use LaTeX to write all text\n","      \"text.usetex\": True,\n","      \"font.family\": \"serif\",\n","      # Use 10pt font in plots, to match 10pt font in document\n","      \"axes.labelsize\": 10,\n","      \"font.size\": 10,\n","      # Make the legend/label fonts a little smaller\n","      \"legend.fontsize\": 8,\n","      \"xtick.labelsize\": 8,\n","      \"ytick.labelsize\": 8,\n","      \"legend.loc\":'lower left'\n","  }\n","\n","  plt.rcParams.update(tex_fonts)\n","\n","  fig, ax = plt.subplots(1, 1, figsize=set_size(width))\n","\n","  rects = ax.bar(ticks, counts, width=.5, align='center')\n","  ax.set(xticks=ticks, xlim=[-1, len(ticks)], title=title, ylabel='Number of papyri',xlabel='Number of fragments per papyri')\n","\n","  def autolabel(rects):\n","    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n","    for rect in rects:\n","      height = rect.get_height()\n","      ax.annotate('{}'.format(height),xy=(rect.get_x() + rect.get_width() / 2, height),xytext=(0, 3), textcoords=\"offset points\",ha='center', va='baseline')\n","\n","  autolabel(rects)\n","\n","  fig.tight_layout()\n","  fig.savefig(fig_name, format='pdf', bbox_inches='tight')\n","  plt.close()\n","  plt.show()"]},{"cell_type":"code","source":["if False:\n","  config_path = './gdrive/MyDrive/mt/conf/conf.toml'\n","  config = toml.load(config_path)\n","  setting = config.get('settings') \n","  logger.info(f' Start Experiment {setting[\"experiment_name\"]}')\n","  param = config.get('params')\n","  dml_param = config.get('dml_params')\n","  output_dir = create_output_dir(setting['output'], setting['experiment_name'])\n","  processed_frame = create_processed_info(setting['path_train'], setting['debug'])\n","  len(processed_frame.papyID.unique())"],"metadata":{"id":"7UDct-pSZRQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ffVk0DmpW8j"},"source":["# Start Experiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjqjvG7eXpxs"},"outputs":[],"source":["def train_and_validate(config_path):\n","  logger.info(f'Initilization  -------------------------')\n","  config = toml.load(config_path)\n","  setting = config.get('settings')\n","  logger.info(f'Experiment:           {setting[\"experiment_name\"]}')\n","  param = config.get('params')\n","  dml_param = config.get('dml_params')\n","  output_dir = create_output_dir(setting['output'], setting['experiment_name'])\n","  device = torch.device(setting['env'])\n","\n","  ###### Log ######\n","  logger.info(f'Debug:                {setting[\"debug\"]}')\n","  logger.info(f'Loos Function:        {dml_param[\"loss_function\"]}')\n","  logger.info(f'Margin Miner Margin:  {dml_param[\"TripletMarginMiner_margin\"]}')\n","  logger.info(f'Triplet Margin Loss:  {dml_param[\"TripletMarginLoss_margin\"]}')\n","  logger.info(f'Type of Tribles:      {dml_param[\"type_of_triplets\"]}')\n","  logger.info(f'Miner:                {dml_param[\"miner\"]}')\n","  logger.info(f'Reducer:              {dml_param[\"reducer\"]}')\n","  logger.info(f'Archi:                {param[\"archi\"]}')\n","  logger.info(f'Epochs:               {param[\"num_epochs\"]}')\n","  logger.info(f'Batch Size:           {param[\"batch_size\"]}')\n","  logger.info(f'Optimizer:            {param[\"optimizer\"]}')\n","  logger.info(f'Learning Rate:        {param[\"lr\"]}')\n","  logger.info(f'Shuffle:              {param[\"shuffle\"]}')\n","  logger.info(f'Padding Width:        {param[\"padding_width\"]}')\n","  logger.info(f'Padding Height:       {param[\"padding_height\"]}')\n","  logger.info(f'Center Crop Width:    {param[\"center_crop_width\"]}')\n","  logger.info(f'Center Crop Height:   {param[\"center_crop_height\"]}')\n","\n","\n","  ###### Transformation ######\n","  '''transform = transforms.Compose([transforms.ToTensor(),\n","                                  transforms.RandomCrop((param['padding_height'],param['padding_width']), padding=None, pad_if_needed=True, fill=0, padding_mode='constant'),\n","                                  transforms.Normalize((param['normalize_0'],param['normalize_1'],param['normalize_2']),(param['normalize_3'],param['normalize_4'], param['normalize_5']))])\n","   '''\n","  transform = transforms.Compose(\n","    [\n","        transforms.Resize((64,64)),\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[param['normalize_0'], param['normalize_1'], param['normalize_2']], std=[param['normalize_3'], param['normalize_4'], param['normalize_5']]),\n","    ])\n","  \n","  ############### Debug or Train Data ###############\n","  if setting['debug']:\n","    processed_frame_train = create_processed_info(setting['path_train'], setting['debug'])\n","    processed_frame_val = create_processed_info(setting['path_val'], setting['debug'])\n","    processed_frame_test = create_processed_info(setting['path_test'])\n","  else:\n","    processed_frame_train = create_processed_info(setting['path_train'])\n","    processed_frame_val = create_processed_info(setting['path_val'])\n","    processed_frame_test = create_processed_info(setting['path_test'])\n","\n","  ############### Datasets ###############\n","  dataset1 = FAUPapyrusCollectionDataset(setting['path_train'], processed_frame_train, transform)\n","  dataset2 = FAUPapyrusCollectionDataset(setting['path_val'], processed_frame_val, transform)\n","  dataset3 = FAUPapyrusCollectionDataset(setting['path_test'], processed_frame_test, transform)\n","\n","  ############### Sampler ###############\n","  train_labels = processed_frame_train.papyID.unique()\n","  val_labels = processed_frame_train.papyID.unique()\n","  test_labels = processed_frame_train.papyID.unique()\n","\n","  train_sampler = samplers.MPerClassSampler(train_labels, 1, batch_size=None, length_before_new_iter=len(dataset1))\n","  val_sampler = samplers.MPerClassSampler(val_labels, 1, batch_size=None, length_before_new_iter=len(dataset2))\n","  test_sampler = samplers.MPerClassSampler(test_labels, 1, batch_size=None, length_before_new_iter=len(dataset3))\n","\n","  ############### Log Distribution ###############\n","  get_cleand_papyri_hist(processed_frame_train, title = 'Histogram Fragments / Papyri  [Train]', fig_name=output_dir + '/HistogramFragAfterTrain.pdf')\n","  get_cleand_papyri_hist(processed_frame_val, title = 'Histogram Fragments / Papyri  [Val]', fig_name=output_dir +  '/HistogramFragAfterVal.pdf')\n","  get_cleand_papyri_hist(processed_frame_test, title = 'Histogram Fragments / Papyri  [Test]', fig_name=output_dir +  '/HistogramFragTest.pdf')\n","\n","  ############### Dataloader ###############\n","  test_loader = torch.utils.data.DataLoader(dataset3, batch_size=param['batch_size'], drop_last=True, sampler=train_sampler)\n","  train_loader = torch.utils.data.DataLoader(dataset1, batch_size=param['batch_size'], shuffle=False, sampler=val_sampler)\n","  val_loader = torch.utils.data.DataLoader(dataset2, batch_size=param['batch_size'], drop_last=True)\n","  \n","  ############### Archi ###############\n","  if param['archi'] == 'SimpleCNN':\n","    model = Net().to(device)\n","  elif param['archi'] == 'efficientnetB0':\n","    model = EfficientNet.from_name('efficientnet-b0').to(device)\n","  elif param['archi'] == 'ResNet':\n","    model = models.resnet18(pretrained=True).to(device)\n","\n","  ############### Optimizer ###############\n","  if param['optimizer'] == 'Adam':\n","    optimizer = optim.Adam(model.parameters(), lr=param['lr'], weight_decay=0.00005)\n","  elif param['optimizer'] == 'SGD': \n","    optimizer =optim.SGD(model.parameters(), lr=param['lr'], momentum=param['sgd_momentum'])\n","  else:\n","    logger.error(' Optimizer is not supported or you have not specified one.')\n","    raise ValueError() \n","\n","  ###############  Distance ###############\n","  if  dml_param['distance'] == 'CosineSimilarity':   \n","    distance = distances.CosineSimilarity()\n","  elif  dml_param['distance'] == 'LpDistance':   \n","    distance = distances.LpDistance(normalize_embeddings=True, p=2, power=1)\n","  else:\n","    logger.error(' Distance is not supported or you have not specified one.') \n","    raise ValueError()\n","\n","  ###############  Reducer ###############\n","  if  dml_param['reducer'] == 'ThresholdReducer':   \n","    reducer = reducers.ThresholdReducer(low=dml_param['ThresholdReducer_low'])\n","  elif  dml_param['reducer'] == 'AvgNonZeroReducer':\n","    reducer = reducers.AvgNonZeroReducer()\n","  else:\n","    logger.error(f'Reducer is not supported or you have not specified one.')\n","    raise ValueError() \n","  \n","  ###############  Loss ###############\n","  if dml_param['loss_function'] == 'TripletMarginLoss': \n","    loss_func = losses.TripletMarginLoss(margin=dml_param['TripletMarginLoss_margin'], distance=distance, reducer=reducer)\n","  elif dml_param['loss_function'] == 'ContrastiveLoss':\n","    loss_func = losses.ContrastiveLoss(pos_margin=1, neg_margin=0)\n","  else:\n","    logger.error(' DML Loss is not supported or you have not specified one.')\n","    raise ValueError() \n","\n","  ############### Mining ###############\n","  if dml_param['miner'] == 'PairMarginMiner':  \n","    mining_func = miners.PairMarginMiner(pos_margin=0.2, neg_margin=0.8)\n","  elif dml_param['miner'] == 'TripletMarginMiner':\n","    mining_func = miners.TripletMarginMiner(\n","        margin=dml_param['TripletMarginMiner_margin'], distance=distance, type_of_triplets=dml_param['type_of_triplets'])\n","  elif dml_param['miner'] == 'UniformHistogramMiner':\n","    mining_func = miners.UniformHistogramMiner(num_bins=100,pos_per_bin=25,neg_per_bin=33,distance=distance)\n","  else:    \n","    logger.error('DML Miner is not supported or you have not specified one.')\n","    raise ValueError() \n","\n","  ############### Accuracy ###############\n","  accuracy_calculator = AccuracyCalculator(include=(dml_param['metric_1'],\n","                                                    dml_param['metric_2'],\n","                                                    dml_param['metric_3'],\n","                                                    dml_param['metric_4']),                                                   \n","                                           k=dml_param['precision_at_1_k'])\n","\n"," \n","\n","\n","  ############### Trainer ###############\n","  train_loss_values = []\n","  val_loss_values = []\n","  val_num_triplets_values = []\n","  val_AMI_values = []\n","  val_NMI_values = []\n","  val_mean_average_precision_values = []\n","  val_precision_at_1_values = []\n","\n","  for epoch in range(1, param['num_epochs'] + 1):\n","      \n","      logger.info(f'Epoch {epoch} -------------------------')\n","\n","      \n","      train_loss, train_num_triplets = train(model, loss_func, mining_func, device, train_loader, optimizer, epoch)\n","      train_loss_values.append(train_loss)\n","\n","      ############### Validation ###############\n","      val_loss, val_num_triplets, val_AMI, val_NMI, val_mean_average_precision, val_precision_at_1 = validation(model, loss_func, mining_func, device, val_loader, optimizer, dataset1, dataset2, accuracy_calculator,epoch)\n","      val_loss_values.append(val_loss)\n","      val_AMI_values.append(val_AMI)\n","      val_NMI_values.append(val_NMI)\n","      val_mean_average_precision_values.append(val_mean_average_precision)\n","      val_precision_at_1_values.append(val_precision_at_1)\n","\n","      ############### Checkpoint ###############            \n","      torch.save({\n","                  'epoch': epoch,\n","                  'model_state_dict': model.state_dict(),\n","                  'optimizer_state_dict': optimizer.state_dict(),\n","                  'loss': train_loss,\n","                  }, output_dir + \"/model \"+ f'_epoch_{str(epoch)}'+ \".pt\")\n","      \n","      ############### Logging ###############            \n","      gradient_visualization(model.named_parameters(), output_dir)\n","      if epoch >= 4:\n","        create_logging(setting, param, dml_param, train_loss_values, val_loss_values, val_AMI_values, val_NMI_values, val_mean_average_precision_values, val_precision_at_1_values, epoch, output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgI4z_3TggrM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639664699458,"user_tz":-60,"elapsed":1051691,"user":{"displayName":"Timo Bohnstedt","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjKtWSKj2quNeSBKu9HJVcxjnTOrJixK-TXw5rjgQ=s64","userId":"17640216031717297977"}},"outputId":"c2082cda-27a2-4dd0-eb7a-0f67cb8501e2"},"outputs":[{"output_type":"stream","name":"stderr","text":["Initilization  -------------------------\n","Initilization  -------------------------\n","Experiment:           Debug\n","Experiment:           Debug\n","Debug:                False\n","Debug:                False\n","Loos Function:        TripletMarginLoss\n","Loos Function:        TripletMarginLoss\n","Margin Miner Margin:  0.02\n","Margin Miner Margin:  0.02\n","Triplet Margin Loss:  1\n","Triplet Margin Loss:  1\n","Type of Tribles:      semihard\n","Type of Tribles:      semihard\n","Miner:                TripletMarginMiner\n","Miner:                TripletMarginMiner\n","Reducer:              AvgNonZeroReducer\n","Reducer:              AvgNonZeroReducer\n","Archi:                ResNet\n","Archi:                ResNet\n","Epochs:               4\n","Epochs:               4\n","Batch Size:           128\n","Batch Size:           128\n","Optimizer:            Adam\n","Optimizer:            Adam\n","Learning Rate:        4e-05\n","Learning Rate:        4e-05\n","Shuffle:              True\n","Shuffle:              True\n","Padding Width:        512\n","Padding Width:        512\n","Padding Height:       512\n","Padding Height:       512\n","Center Crop Width:    512\n","Center Crop Width:    512\n","Center Crop Height:   512\n","Center Crop Height:   512\n","Epoch 1 -------------------------\n","Epoch 1 -------------------------\n"," Training:\n"," Training:\n","  Mined Tiplets 150\n","  Mined Tiplets 150\n","  Loss 0.9892277121543884\n","  Loss 0.9892277121543884\n"," Validation:\n"," Validation:\n","  Mined Tiplets: 891\n","  Mined Tiplets: 891\n","  Loss0.9898995161056519\n","  Loss0.9898995161056519\n","100%|██████████| 88/88 [02:33<00:00,  1.75s/it]\n","100%|██████████| 11/11 [00:17<00:00,  1.58s/it]\n","  AMI 0.05612803372201111\n","  AMI 0.05612803372201111\n","  NMI 0.668872591030951\n","  NMI 0.668872591030951\n","  MAP 0.19680851063829788\n","  MAP 0.19680851063829788\n","  P@1 0.19680851063829788\n","  P@1 0.19680851063829788\n","Epoch 2 -------------------------\n","Epoch 2 -------------------------\n"," Training:\n"," Training:\n","  Mined Tiplets 154\n","  Mined Tiplets 154\n","  Loss 0.9895154237747192\n","  Loss 0.9895154237747192\n"," Validation:\n"," Validation:\n","  Mined Tiplets: 790\n","  Mined Tiplets: 790\n","  Loss0.9896195530891418\n","  Loss0.9896195530891418\n","100%|██████████| 88/88 [02:34<00:00,  1.75s/it]\n","100%|██████████| 11/11 [00:17<00:00,  1.58s/it]\n","  AMI 0.06771121309947525\n","  AMI 0.06771121309947525\n","  NMI 0.675534551103451\n","  NMI 0.675534551103451\n","  MAP 0.17553191489361702\n","  MAP 0.17553191489361702\n","  P@1 0.17553191489361702\n","  P@1 0.17553191489361702\n","Epoch 3 -------------------------\n","Epoch 3 -------------------------\n"," Training:\n"," Training:\n","  Mined Tiplets 118\n","  Mined Tiplets 118\n","  Loss 0.989776074886322\n","  Loss 0.989776074886322\n"," Validation:\n"," Validation:\n","  Mined Tiplets: 737\n","  Mined Tiplets: 737\n","  Loss0.9898122549057007\n","  Loss0.9898122549057007\n","100%|██████████| 88/88 [02:31<00:00,  1.73s/it]\n","100%|██████████| 11/11 [00:17<00:00,  1.60s/it]\n","  AMI 0.04579869780598331\n","  AMI 0.04579869780598331\n","  NMI 0.670741873979595\n","  NMI 0.670741873979595\n","  MAP 0.13829787234042554\n","  MAP 0.13829787234042554\n","  P@1 0.13829787234042554\n","  P@1 0.13829787234042554\n","Epoch 4 -------------------------\n","Epoch 4 -------------------------\n"," Training:\n"," Training:\n","  Mined Tiplets 87\n","  Mined Tiplets 87\n","  Loss 0.9889822602272034\n","  Loss 0.9889822602272034\n"," Validation:\n"," Validation:\n","  Mined Tiplets: 727\n","  Mined Tiplets: 727\n","  Loss0.9895182251930237\n","  Loss0.9895182251930237\n","100%|██████████| 88/88 [02:31<00:00,  1.73s/it]\n","100%|██████████| 11/11 [00:17<00:00,  1.60s/it]\n","  AMI 0.045248547879339324\n","  AMI 0.045248547879339324\n","  NMI 0.6763377222582652\n","  NMI 0.6763377222582652\n","  MAP 0.10638297872340426\n","  MAP 0.10638297872340426\n","  P@1 0.10638297872340426\n","  P@1 0.10638297872340426\n"]}],"source":["train_and_validate(config_path = './gdrive/MyDrive/mt/conf/conf.toml')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"unb2c9nSsuA_"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"0.1.1-tb-train.ipynb","provenance":[{"file_id":"1cji1rIdM4RRwBlhJGv7lxJM4WBjEr4Zp","timestamp":1639509376608},{"file_id":"https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/examples/notebooks/MoCoCIFAR10.ipynb","timestamp":1637922532820}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b0e097be3bcf4a92b2f1506f7d3c47af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f73fbafeb8c54eb68d8e5692a16fcca6","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_85febc5e968443509ba3731e0b2da29d","IPY_MODEL_7a5dea9d8c364977a298b61f9d5c9f28","IPY_MODEL_7aa2051d96da4ebbb2a30ef53415b11d"]}},"f73fbafeb8c54eb68d8e5692a16fcca6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"85febc5e968443509ba3731e0b2da29d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3d89c5297c9b4dfeaae4df9f12965010","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_18c945feefda466e826e400fba7eec02"}},"7a5dea9d8c364977a298b61f9d5c9f28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_54a2198b800b4bb8aba3c43329574a08","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5883a7227b8342588c5abf0b2a753b8e"}},"7aa2051d96da4ebbb2a30ef53415b11d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_596892744ef04c3ea227e04f9c1062fa","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/? [00:17&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_df7a6a90b7ad4ef09710133ea17324c4"}},"3d89c5297c9b4dfeaae4df9f12965010":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"18c945feefda466e826e400fba7eec02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"54a2198b800b4bb8aba3c43329574a08":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"5883a7227b8342588c5abf0b2a753b8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":"20px","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"596892744ef04c3ea227e04f9c1062fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"df7a6a90b7ad4ef09710133ea17324c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"nbformat":4,"nbformat_minor":0}